{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArmAIF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJUJ2oEmkwgg5hmPDR+Rlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rfrowein/armAI/blob/main/ArmAIF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhRVYrwfC5OT"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "from matplotlib.transforms import Bbox\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import imageio\n",
        "\n",
        "from statistics import mean\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import cv2\n",
        "from skimage.util import random_noise\n",
        "from PIL import Image\n",
        "\n",
        "#create folder /data in current working directory, if it does not exist yet\n",
        "if not os.path.exists(os.getcwd()+'/data'):\n",
        "    os.mkdir(os.getcwd()+'/data')\n",
        "\n",
        "\n",
        "#Create folder /networks in current working directory, if it does not exist yet\n",
        "if not os.path.exists(os.getcwd()+'/networks'):\n",
        "        os.mkdir(os.getcwd()+'/networks')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpEQ2OnrD8Rs"
      },
      "source": [
        "'''\n",
        "Info: Create an environment containing an arm\n",
        "\n",
        "Input:\n",
        "  -position: Location of the arm in the environment [-1,1]\n",
        "  -name: Name of the newly created figure (.png,.jpg, etc. is not needed)\n",
        "\n",
        "Output: -- save figure --\n",
        "'''\n",
        "def create_image_v2(position, name):\n",
        "    plt.style.use('dark_background') #Change the style of ALL plots to black background [1]\n",
        "\n",
        "    image = mpimg.imread('arm.png')\n",
        "    \n",
        "    #frameon = false, removes the outer axis --> needed as when turning axis('off') will also remove background\n",
        "    fig = plt.figure(figsize=(2,1.5),frameon=False)\n",
        "    \n",
        "    #Create environment, y:[0, 1.5] x:[0, 2.7] (the 2.7 is the -1,1 environment + 0.7 of arm width preventing out of bounds)\n",
        "    ax1 = fig.add_axes([0, 0, 2.7, 1.5])\n",
        "    \n",
        "    #Create arm, which is plotted inside the environment\n",
        "    ax2 = fig.add_axes([(position+1), 0, 0.7, 1]) #as the plot runs from 0 to 2 instead of -1 to 1, +1 is added\n",
        "    ax2.axis('off')\n",
        "    ax2.imshow(image, aspect='auto', zorder=-1) #add image\n",
        "    \n",
        "    #Variable extent is needed to prevent padding from forming when saving. \n",
        "      # -as the saved image is used for the neural network padding breaks it (when using a black background)\n",
        "    #Bbox_inches = 'tight' will leave padding\n",
        "    #Bbox_inches = 0 will leave no usable image\n",
        "    #Pad_inches = 0 will leave padding, setting it to -0.32 will remove padding, but deform image (altough still usable)\n",
        "    extent = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted()) \n",
        "    plt.savefig(name + '.png',bbox_inches=extent)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGepyMP_JTP6"
      },
      "source": [
        "'''\n",
        "Info: Create image, read image (add noise) and transform it to a tensor\n",
        "\n",
        "Input:\n",
        "  -name: String containing the name of the to be saved image (exclude .png)\n",
        "  -location (optional): Single x-axis positional input\n",
        "  -noise: True/False for adding Gaussian noise to the image (0.01)\n",
        "\n",
        "Outout: img_tensor , location\n",
        "  -img_tensor: Tensor of the image\n",
        "  -location: The x location of the image\n",
        "'''\n",
        "\n",
        "def create_tensor_v2(name, location = None, noise = True):\n",
        "    \n",
        "    if location is None:\n",
        "        location = round(random.uniform(-1, 1),2) # Create random location between -1 and 1 (2 decimal)\n",
        "        \n",
        "    create_image_v2(location,name) # Create image\n",
        "    \n",
        "    #Create and read image\n",
        "    img = cv2.imread(name + '.png',0)\n",
        "    img = cv2.resize(img, (40,40))\n",
        "    if noise:\n",
        "      img = random_noise(img,mode='gaussian')\n",
        "    \n",
        "    #Convert image to tensor\n",
        "    img_tensor = torch.from_numpy(img)\n",
        "    return img_tensor, location  #converts the nparray image to tensor (I,X)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdnTbsHlN-Ef"
      },
      "source": [
        "'''\n",
        "Info: Create randomly generated data (grayscale) and save it in folder /data (folder is created if not present)\n",
        "\n",
        "Input:\n",
        "  -nr_data (optional): number of randomly generated data\n",
        "\n",
        "Output: tuple(list[location], list[img_tensor])\n",
        "  -data_X: List of he x locations of the images\n",
        "  -data_I: Images as tensor\n",
        "\n",
        "'''\n",
        "\n",
        "def create_data_v2(nr_data = 100, noise = True):\n",
        "    data_X = []\n",
        "    data_I = []\n",
        "    \n",
        "    #Create random data\n",
        "    for i in range(0,nr_data,1):\n",
        "        I,X = create_tensor_v2(os.getcwd()+'/data/true_image'+str(i), noise = noise) #location is excluded to get random locations\n",
        "        data_X.append(torch.FloatTensor([X]))\n",
        "        data_I.append(I/255) # The I/255 is a conversion from RGB to grayscale \n",
        "        plt.close('all') #Close all plots and/or images (precaution for memory build up)\n",
        "    return (data_X, data_I)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e1zU11BQLOI"
      },
      "source": [
        "'''\n",
        "Info: Neural network using 4 transposed convolutional layers to generate an image from a single horizontal positional location\n",
        "\n",
        "Input: Single x locational variable\n",
        "Output: 40 x 40 image\n",
        "\n",
        "'''\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.epoch_losses = []\n",
        "        self.test_losses = []\n",
        "        self.saved_lr = []\n",
        "        self.saved_batch_size = []\n",
        "        \n",
        "        # The decoder uses 4 layeres, where 2 have rectangular kernels \n",
        "        super(Net,self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1 , 16, (1,10)),  #input (*batch_size*,1,1,1) output (*batch_size*,16,1,10)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16 , 8, (10,1)), #input (*batch_size*,16,1,10) output (*batch_size*,8,10,10)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8 , 4 , 9,stride=2), #input (*batch_size*,8,10,10) output (*batch_size*,4,27,27)\n",
        "            nn.ReLU(True),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.ConvTranspose2d(4 , 1 , 14), #input (*batch_size*,4,27,27) output (*batch_size*,1,40,40)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    '''\n",
        "    Info: Evaluate the current model (uses entire dataset as batch)\n",
        "    Input:\n",
        "      -test_data (optional): tuple(list[x_location], list[image])\n",
        "    Output:\n",
        "      -test_loss: Mean squared error loss of the test_data\n",
        "    '''\n",
        "    def eval_model(self,test_data=None):\n",
        "        model.eval()#set model to evaluation mode\n",
        "        loss_func = nn.MSELoss()\n",
        "\n",
        "        if test_data==None:\n",
        "            X,I = create_data_v2(self.batch_size)\n",
        "            test_data = (X,I)   \n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_input = Variable(torch.stack(test_data[0]).view(len(test_data[0]),1,1,1)) \n",
        "            \n",
        "            test_output = model.decoder(test_input)\n",
        "            test_original = torch.stack(test_data[1]).view(len(test_data[1]),1,40,40)\n",
        "                       \n",
        "            loss = loss_func(test_output, test_original.type(torch.FloatTensor))\n",
        "            test_loss = loss.item()\n",
        "            return test_loss\n",
        "    \n",
        "    # Main function, call to run the model, trains and tests the current model\n",
        "        # Input:\n",
        "        # - data (optional): Training data as tuple (tensorlist locations, tensorlist images)\n",
        "        # - epoch (optional): Number of cycles to run over the training (and test) data\n",
        "        # - plot (optional): True/False, if True will plot information and prrogress every 10 epochs\n",
        "        # - batch_size (optional): Set the batch size of the data --> if batch_size incompatible with data_size, data_size will reduce to fit in full batches\n",
        "        # Output:\n",
        "        # - epoch_losses: List of train losses from each epoch\n",
        "        # - test_losses: List of test losses from each epoch\n",
        "\n",
        "    '''\n",
        "    Info: Train and test the neural network (testing data is created on top of the training data, 30%)\n",
        "      -The neural network uses a schedular decreasing learning rate over time (based on ReduceLROnPlateau)\n",
        "    Input:\n",
        "      -data (optional): training data as tuple(list[x-location],list[image])\n",
        "      -epochs (optional): number of cycles to run over the data\n",
        "      -plot: True/False for plotting progress during runtime (increases runtime)\n",
        "      -batch_size (optional): number of data to run trough before updating internal parameters\n",
        "      -hybrid: True/False for increasing batch_size during runtime (increase every 10 epochs)\n",
        "    Output:\n",
        "      -epoch_losses: Mean squared error loss of each cycle (mean of MSE of the batches)\n",
        "      -test_losses: Mean squared error loss of each cycle running oveer the test data\n",
        "    '''\n",
        "    def train_model(self, data=None, epochs=20 , plot=True, batch_size = 16, hybrid = True):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor = .8)\n",
        "        loss_func = nn.MSELoss()        \n",
        "\n",
        "        # Create random data if none is given\n",
        "        if data is None:\n",
        "            X,I = create_data_v2(500)\n",
        "            data = (X,I)\n",
        "\n",
        "        # Create random evaluation/test data (with a ratio of about 30% of the total data)\n",
        "        X_eval, I_eval = create_data_v2(math.floor((len(data)/0.7) * 0.3),noise = False)\n",
        "        data_eval = (X_eval, I_eval)\n",
        "\n",
        "        \n",
        "        #train and test model\n",
        "        for epoch in range(epochs):\n",
        "            for param_group in optimizer.param_groups:\n",
        "                self.saved_lr.append(param_group['lr'])\n",
        "\n",
        "            if epoch%10 == 0 and hybrid:\n",
        "                self.batch_size+=1\n",
        "            self.saved_batch_size.append(self.batch_size) #save batch size\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            #shuffle train and test data at start of each epoch\n",
        "            comb_data = list(zip(data[0], data[1])) # Combine X,I (keep the X with its attached I when shuffeling)\n",
        "            comb_data_eval = list(zip(data_eval[0], data_eval[1]))\n",
        "            random.shuffle(comb_data)\n",
        "            random.shuffle(comb_data_eval)\n",
        "            X, I = zip(*comb_data)\n",
        "            X_eval, I_eval = zip(*comb_data)\n",
        "\n",
        "            model.train() #Set model to train mode\n",
        "\n",
        "            \n",
        "\n",
        "            for i in (range(math.floor(len(X)/self.batch_size))): # turn the data in to batches of batch_size (rounded downwards)\n",
        "                input = Variable(torch.stack(X[i * self.batch_size:(i + 1) * self.batch_size]).view(self.batch_size,1,1,1), requires_grad=True)\n",
        "                original = torch.stack(I[i * self.batch_size:(i + 1) * self.batch_size]).view(self.batch_size,1,40,40)\n",
        "                output = model.forward(input)\n",
        "              \n",
        "                #Forward pass\n",
        "                optimizer.zero_grad() #zero the gradient buffers\n",
        "                loss = loss_func(output, original.type(torch.FloatTensor)) # Mean Squared Error (MSE) loss\n",
        "               \n",
        "                #Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()  #update optimizer\n",
        "            \n",
        "                batch_losses.append(loss.item()) \n",
        "                output = output.detach() #This is done for plotting purposes\n",
        "            test_loss = self.eval_model((X_eval, I_eval)) #Set model to evaluation mode\n",
        "            \n",
        "            self.test_losses.append(test_loss)\n",
        "            epoch_loss = mean(batch_losses)\n",
        "            self.epoch_losses.append(epoch_loss)\n",
        "        \n",
        "            scheduler.step(epoch_loss) #update schedular\n",
        "        \n",
        "            # Plot information about the training and testing of the model during runtim if plot is true, otherwise a single plot at the end is shown\n",
        "            if (epoch % 10 == 0 and plot==True and epoch !=0) or epoch == epochs-1:\n",
        "                clear_output(wait=True) #Clear output field\n",
        "                \n",
        "                #Indicate losses of current cycle\n",
        "                print('epoch [{}/{}]\\nepoch loss: {}\\ntest loss: {}\\n'.format(epoch+1,epochs,epoch_loss,test_loss))        \n",
        "\n",
        "                #Show all losses\n",
        "                print('Loss plot (excluding first 5 epochs)')\n",
        "                x1 = np.linspace(5,len(self.epoch_losses),len(self.epoch_losses)-5,endpoint=True)\n",
        "                x2 = np.linspace(5,len(self.test_losses),len(self.test_losses)-5,endpoint=True)\n",
        "                plt.plot(x1,self.epoch_losses[5::],'w',label='train')\n",
        "                plt.plot(x2,self.test_losses[5::],'r--',label='test', alpha = 0.5)\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.xlabel('epoch')\n",
        "                plt.ylabel('MSE')\n",
        "                plt.xlim(xmin=5)\n",
        "                plt.show()\n",
        "                plt.clf()\n",
        "\n",
        "                #Show first/a random data generation from the neural network (shows progress)\n",
        "                print('\\nVisualization\\n epoch: {}\\n batch_size: {}'.format(epoch,self.batch_size))\n",
        "                fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(3, 5))\n",
        "                ax[0].imshow(output[0][0].view(40,40), origin='upper', cmap='gray')\n",
        "                ax[0].set_title('predicted')\n",
        "                ax[0].axis('off')\n",
        "                ax[1].imshow(original[0][0].view(40,40), origin='upper', cmap='gray')\n",
        "                ax[1].set_title('original')\n",
        "                ax[1].axis('off')\n",
        "                plt.show()\n",
        "                plt.clf()\n",
        "        return self.epoch_losses, self.test_losses\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDH3AVmRaeuD"
      },
      "source": [
        "'''\n",
        "Info: Plot information (losses, learning rate, batch size) about trained neural network\n",
        "'''\n",
        "\n",
        "'''\n",
        "Input:\n",
        "  -model: trained neural network\n",
        "  -reduce_start (optional): Set starting epoch for the plot\n",
        "  -reduce_end (optional): Remove N amount of epochs from the end\n",
        "Output: --Plot of train and test losses --\n",
        "'''\n",
        "def visualize_learning(model, reduce_start = 0, reduce_end = 0):\n",
        "  #calculate amount of epochs\n",
        "  len_data = np.linspace(reduce_start,len(model.epoch_losses)-reduce_end,len(model.epoch_losses)-reduce_start-reduce_end,endpoint = True)\n",
        "  #plot train losses\n",
        "  plt.plot(len_data,model.epoch_losses[reduce_start:len(model.epoch_losses)-reduce_end], 'b', label = 'train')\n",
        "  #plot test losses\n",
        "  plt.plot(len_data,model.test_losses[reduce_start:len(model.test_losses)-reduce_end], 'r', label = 'test')\n",
        "\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('MSE')\n",
        "  plt.xlim(xmin=reduce_start)\n",
        "  plt.title('epoch losses')\n",
        "  plt.show()\n",
        "\n",
        "'''\n",
        "Input:\n",
        "  -model: trained neural network\n",
        "Output: --Plot of learning rate and a plot of batch sizes--\n",
        "'''\n",
        "def visualize_params(model):\n",
        "    x = np.linspace(0,len(model.saved_lr),len(model.saved_lr),endpoint=True)\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "    ax1.plot(x, model.saved_lr)\n",
        "    ax1.set_title('learning rate')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('lr')\n",
        "    ax2.plot(x, model.saved_batch_size)\n",
        "    ax2.set_title('batch size')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.set_ylabel('size')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtpCaFXge8i8"
      },
      "source": [
        "Only run the next 2/3 cells if you want to create your own neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr4X4DKMcRB0"
      },
      "source": [
        "'''\n",
        "Info: Create training data for the neural network (large dataset recommended)\n",
        "  -Only run once\n",
        "'''\n",
        "train_data_large = create_data_v2(1600, noise=True)\n",
        "#train_data_medium = create_data_v2(800, noise = True)\n",
        "#train_data_small = create_data_v2(200, noise = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878kszUmcu8u"
      },
      "source": [
        "'''\n",
        "Info: Create, run and save neural network\n",
        "  -There is a small possibility the network gets stuck\n",
        "'''\n",
        "model = Net() #create network\n",
        "\n",
        "#In the case of hybrid it is advisary to start with a small batch size\n",
        "train_loss, test_loss = model.train_model(data=train_data_large, batch_size=1, epochs=800, hybrid = True) #run network\n",
        "\n",
        "#Save trained neural network,\n",
        "torch.save(model, os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuO8OoERdw3g"
      },
      "source": [
        "'''\n",
        "Info: Show additional/more specific results\n",
        "'''\n",
        "#model = torch.load(os.getcwd()+'/networks/trained_network_Datalarge_Hybrid.pth')  \n",
        "#visualize_learning(model,10,0)\n",
        "#visualize_params(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzU4sblWegN7"
      },
      "source": [
        "'''\n",
        "Info: Save the states created by armAI (active inference)\n",
        "\n",
        "Input:\n",
        "  -armAI: The armAI containing the values needed for plotting\n",
        "  -itt: The current itteration/state\n",
        "  -visual: True/False for plotting sensory visual state\n",
        "  -proprioception: True/False for plotting sensory proprioception state\n",
        "  -goal: True/False for plotting goal state\n",
        "Output: -- image of the armAI state --\n",
        "'''\n",
        "\n",
        "def save_state_v4(armAI, itt, visual, proprioception, goal):\n",
        "\n",
        "    legend = []\n",
        "    #get image to be plotted as indicators\n",
        "    image = mpimg.imread('arm.png')\n",
        "\n",
        "    #Create figure\n",
        "    fig = plt.figure(figsize=(2,1.5),frameon=False)\n",
        "\n",
        "    # Create environment\n",
        "    env = fig.add_axes([0, 0, 2.7, 1.5],alpha=0.5,facecolor='black') # The 2.7 is the -1,1 environment + 0.7 of arm width (that the arm at outer location -1 and 1 are still within the environment)\n",
        "    env.get_yaxis().set_visible(False)\n",
        "    env.get_xaxis().set_visible(False)\n",
        "\n",
        "    #Add belief/mental (blue)\n",
        "    mental_x_clipped = np.clip(armAI.mental_x,-1,1)\n",
        "    belief = fig.add_axes([(mental_x_clipped+1), 0, 0.7, 1],facecolor = 'b')\n",
        "    belief.patch.set_alpha(0.2)\n",
        "    belief.get_yaxis().set_visible(False)\n",
        "    belief.get_xaxis().set_visible(False)\n",
        "    belief.spines['bottom'].set_color('b')\n",
        "    belief.spines['top'].set_color('b') \n",
        "    belief.spines['right'].set_color('b')\n",
        "    belief.spines['left'].set_color('b')\n",
        "    belief.patch.set_alpha(0.5)\n",
        "    belief_leg = mpatches.Patch(color='b', label='Mental | ' + str(round(armAI.mental_x.item(), 3)))\n",
        "    legend.append(belief_leg)\n",
        "    \n",
        "    #Use the generated mental state as plot for belief\n",
        "    belief_img = armAI.network.decoder(Variable(armAI.mental_x.view(-1,1,1,1), requires_grad=False)).detach()\n",
        "    env.imshow(belief_img.view(40,40),cmap = 'gray',aspect='auto')\n",
        "\n",
        "    #Use the arm.png as plot for belief\n",
        "    #belief.imshow(image, aspect='auto', alpha=0.5)\n",
        "\n",
        "    # Add visual arm (red)\n",
        "    if visual:\n",
        "       visual_x_clipped = np.clip(armAI.visual_x,-1,1) #prevent plotting out of bounds\n",
        "       vis = fig.add_axes([(visual_x_clipped+1), 0, 0.7, 1],facecolor='r') # the +1 as the environment runs from 0 - 2\n",
        "       vis.patch.set_alpha(0.2) #give 'faint' background collor\n",
        "       vis.imshow(image, aspect='auto',alpha=0.8)\n",
        "       vis.get_yaxis().set_visible(False)\n",
        "       vis.get_xaxis().set_visible(False)\n",
        "       vis.spines['bottom'].set_color('red')\n",
        "       vis.spines['top'].set_color('red') \n",
        "       vis.spines['right'].set_color('red')\n",
        "       vis.spines['left'].set_color('red')\n",
        "       vis.patch.set_alpha(0.5)\n",
        "       vis_leg = mpatches.Patch(color='red', label='Visual | ' + str(round(armAI.visual_x.item(),3)))\n",
        "       legend.append(vis_leg)\n",
        "\n",
        "    #Add proprioception circle (yellow)\n",
        "    if proprioception: \n",
        "       prop_x_clipped = np.clip(armAI.proprioception_loc,-1,1) #prevent plotting out of bounds\n",
        "       prop = fig.add_axes([(prop_x_clipped+1), 0, 0.7, 1]) # the +1 as the environment runs from 0 - 2\n",
        "       circ = Circle((0.35,0.5),0.1, color = 'yellow')\n",
        "       prop.add_patch(circ)\n",
        "       prop.patch.set_alpha(0.5)\n",
        "       prop.axis('off')\n",
        "       prop_leg = mpatches.Patch(color='yellow', label='Prop | ' + str(round(armAI.proprioception_loc.item(),3)))\n",
        "       legend.append(prop_leg)\n",
        "    \n",
        "    #Add goal (green)\n",
        "    if goal: \n",
        "        goal_x_clipped = np.clip(armAI.goal_x,-1,1) #prevent plotting out of bounds\n",
        "        attr = fig.add_axes([(goal_x_clipped+1), 0, 0.7, 1],facecolor='g' )\n",
        "        attr.get_yaxis().set_visible(False)\n",
        "        attr.get_xaxis().set_visible(False)\n",
        "        attr.spines['bottom'].set_color('g')\n",
        "        attr.spines['top'].set_color('g') \n",
        "        attr.spines['right'].set_color('g')\n",
        "        attr.spines['left'].set_color('g')\n",
        "        attr.patch.set_alpha(0.5) \n",
        "        attr.imshow(image, aspect='auto', alpha=0.5)\n",
        "        attr.patch.set_alpha(0.2)\n",
        "        attr_leg = mpatches.Patch(color='g', label='Goal | ' + str(round(armAI.goal_x.item(),3)))\n",
        "        legend.append(attr_leg)\n",
        "\n",
        "    #Set title and legend\n",
        "    env.text(.5,.9, 'State: ' + str(itt) , horizontalalignment='center', transform=env.transAxes)\n",
        "    env.legend(handles=legend)\n",
        "\n",
        "    plt.savefig(os.getcwd()+'/data/state: '+ str(itt) + '.png', bbox_inches='tight')\n",
        "    return fig\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG1Gl63gjfvt"
      },
      "source": [
        "'''\n",
        "Info: Helper functions for armAI (preventing excessive large cells)\n",
        "'''\n",
        "\n",
        "class AI_helper():\n",
        "    def __init__(self, armAI):\n",
        "      self.armAI = armAI\n",
        "\n",
        "    #Info: check for neural network, load 'trained_network_DataLarge_Hybrid' if none given\n",
        "    def check_network(self):\n",
        "      if self.armAI.network == None:\n",
        "          print('No neural network was given. Searching in folder networks for \\'trained_network_DataLarge_Hybrid.pth\\'')\n",
        "          if os.path.exists(os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth') == False:\n",
        "              print('No network found, please run the \\'Train, Test and Save model\\' section ')\n",
        "              sys.exit()\n",
        "          return torch.load(os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth')    \n",
        "      else:\n",
        "          return self.armAI.network\n",
        "    \n",
        "    #Info: Check if the initialization is correct\n",
        "    def check_initialization(self):\n",
        "        if len(self.armAI.visual) == 0 or self.armAI.mental_x == None or (self.armAI.induce_movement and len(self.armAI.attractor_img) ==  0):\n",
        "            print('\\nINITIALIZATION INCOMPLETE: check set_mental(), set_visual() and/or set_attractor()')\n",
        "            print('vis: {}\\nmental: {}\\nattr: {}'.format(len(self.armAI.visual), self.armAI.mental_x , len(self.attractor_img)))\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    #Info: Create a new visual image based on new_location\n",
        "    def update_visual_state(self, new_location):\n",
        "        self.armAI.visual_prop = new_location\n",
        "        img,loc = create_tensor_v2('visual_state', new_location, noise = False)\n",
        "        self.armAI.set_visual(torch.FloatTensor([loc]),img/255)\n",
        "        plt.clf()\n",
        "\n",
        "    #Info: Add random normal Gaussian noise to value    \n",
        "    def add_noise(self, value, noise):\n",
        "        return value + np.random.normal(0, noise)\n",
        "\n",
        "    #Info: Reform the mu data and visual states as .gif\n",
        "    def save_gif(self, name, dyn):\n",
        "        img_list = []\n",
        "        for i in range(len(self.armAI.mu_dot_data)):\n",
        "            fig, ax = plt.subplots(nrows=2, figsize = (10,9))\n",
        "            ax[1].plot(range(i), self.armAI.mu_dot_data[0:i], 'r', label='mu_dot | ' + str(round(self.armAI.mu_dot_data[i].item(),3)))\n",
        "            if dyn:\n",
        "                ax[1].plot(range(i), self.armAI.a_dot_data[0:i], 'g--', label = 'a_dot | ' + str(round(self.armAI.a_dot_data[i].item(),3)))\n",
        "            ax[1].set_xlabel('itteration')\n",
        "            ax[1].set_title('mu')\n",
        "            ax[1].legend(loc='lower center', fontsize='x-large')\n",
        "            ax[1].plot(range(i),np.zeros(i),'b--')\n",
        "\n",
        "            state = Image.open(os.getcwd()+'/data/state: '+ str(i) + '.png')\n",
        "            ax[0].imshow(state)\n",
        "            ax[0].set_xlabel('location')\n",
        "            ax[0].set_title('Environment')\n",
        "            ax[0].axis('off')\n",
        "\n",
        "            fig.canvas.draw()\n",
        "            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
        "            image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            plt.close(fig)\n",
        "\n",
        "            img_list.append(image)\n",
        "\n",
        "\n",
        "        kwargs_write = {'fps':1.0, 'quantizer':'nq'}\n",
        "        imageio.mimsave(str(name) + '.gif', img_list, fps=2)\n",
        "        #img_list[0].save('temp.gif', save_all = True, optimize = False, duration = 100, loop = 0)\n",
        "        #img_list[0].save('States_all_v2.gif', save_all=True, append_images=img_list[1:], optimize=False, duration=2000, loop=0)\n",
        "\n",
        "          \n",
        "\n",
        "      \n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUnOf-XKm0F0"
      },
      "source": [
        "'''\n",
        "Info: armAI, active inference | Using 2 sensory inputs, internal state (and goal state) to minimize prediction errors.\n",
        "  -perceptual minimization\n",
        "  -action minimization\n",
        "  --goal minimization\n",
        "\n",
        "Input:\n",
        "  -Starting values\n",
        "  -limitations\n",
        "\n",
        "Output: -- .gif containing |all states| process of reaching equilibrium\n",
        "'''\n",
        "class ArmAI():\n",
        "    def __init__(self,max_itt=100, network = None):\n",
        "        \n",
        "      #INITIALIZE CLASS VARIABLES\n",
        "        self.dt = 0.1\n",
        "        self.max_itt = max_itt\n",
        "        self.network = network\n",
        "        self.AI_helper = AI_helper(self)\n",
        "        self.finished = False\n",
        "        \n",
        "        #Sigma's\n",
        "        self.sigma_vis= 0.1\n",
        "        self.sigma_prop = 0.05\n",
        "        self.sigma_dyn = 0.1\n",
        "        self.sigma_dyn_prop = 0.05\n",
        "        \n",
        "        #adap sigma\n",
        "        self.adapt_sigma_vis = True \n",
        "        self.sigma_v_gamma = 0.1\n",
        "\n",
        "        #Gain's\n",
        "        self.action_gain = 0.05\n",
        "        self.perception_gain = 0.05\n",
        "        self.goal_gain = 0.05\n",
        "        \n",
        "        #Error data (plotting purposes)\n",
        "        self.error_vis = [] \n",
        "        self.error_prop = []\n",
        "        self.error_mental = []\n",
        "        self.error_mental_prop = []\n",
        "        self.mu_dot_data = []\n",
        "        self.a_dot_data = []\n",
        "\n",
        "        #Visual (perception)\n",
        "        self.visual_img = torch.FloatTensor() #visual_state\n",
        "        self.visual_x = 0 #visual_location (plotting purposes)\n",
        "        self.pred_err_vis = torch.FloatTensor()\n",
        "        self.pred_err_vis_prior = torch.FloatTensor()\n",
        "\n",
        "        #Proprioception (perception)\n",
        "        self.proprioception_loc = 0\n",
        "\n",
        "        #Mental/belief\n",
        "        self.mental_img = torch.FloatTensor() #mental state\n",
        "        self.mental_x = 0 #mental location/proprioceptive location\n",
        "\n",
        "        #Attractor/goal\n",
        "        self.goal_img = torch.FloatTensor()\n",
        "        self.goal_x = 0 #attractor location\n",
        "      \n",
        "      #CHECK FOR NETWORK\n",
        "        #initialize network\n",
        "        self.network = self.AI_helper.check_network() \n",
        "        #set network to evaluation mode\n",
        "        self.network.eval()\n",
        "        \n",
        "    #SET CLASS VARIABLES\n",
        "    def set_mental(self,belief_loc): #Mental/Belief position\n",
        "        self.mental_x = belief_loc\n",
        "        \n",
        "    def set_goal(self, goal_loc, goal_vis):#Goal position\n",
        "        self.goal_vis = goal_vis\n",
        "        self.goal_x = goal_loc \n",
        "\n",
        "    def set_visual(self, visual_x, visual_img):#Sensory visual\n",
        "        self.visual_img = visual_img\n",
        "        self.visual_x = visual_x #(plotting/visualizing purposes)\n",
        "\n",
        "    def set_proprioception(self, prop_loc): #Sensory proprioception\n",
        "        self.proprioception_loc = prop_loc\n",
        "    \n",
        "    #Visual error (error based on vision)\n",
        "    def visual_error(self, input_image):\n",
        "        #Generate mental/belief image\n",
        "        input = Variable(self.mental_x.view(-1,1,1,1), requires_grad=True)           \n",
        "        self.mental_img = self.network.decoder(input)\n",
        "    \n",
        "        #Error visual\n",
        "        error_vis = 1/self.sigma_dyn * (input_image - self.mental_img) \n",
        " \n",
        "        #Backward pass\n",
        "        input.grad = torch.zeros(input.size())\n",
        "        self.mental_img.backward(error_vis, retain_graph=True)\n",
        "\n",
        "        return input.grad\n",
        "\n",
        "    #Locational/proprioception error (error based on horizontal locations)\n",
        "    def prop_error(self,input_prop):\n",
        "        error_prop = 1/self.sigma_prop * (input_prop - self.mental_x)\n",
        "        return error_prop\n",
        "\n",
        "## The folowing visual, dynamical_visual, proprioception, dynamical_prop can be used if the precision for each section is different\n",
        "## -- capable of saving errors individually\n",
        "## -- are not used in the current run --\n",
        "    #VISUAL/graphical\n",
        "    def visual(self): \n",
        "        #Generate mental/belief image\n",
        "        input = Variable(self.mental_x.view(-1,1,1,1), requires_grad=True)\n",
        "        self.mental_img = self.network.decoder(input)\n",
        "    \n",
        "        #Error visual\n",
        "        error_vis = (self.visual_img - self.mental_img) #visual image - belief image\n",
        "        self.pred_err_vis = error_vis.detach()\n",
        "        #self.error_vis.append(torch.mean(error_vis)*1600) #save error, the *1600 because it is a 40x40 image\n",
        "\n",
        "        dg_dmu_vis=(1/self.sigma_vis)* error_vis\n",
        " \n",
        "        ## Backward pass\n",
        "        input.grad = torch.zeros(input.size())\n",
        "        self.mental_img.backward(dg_dmu_vis, retain_graph=True)\n",
        "        return input.grad\n",
        "\n",
        "    def dynamical_visual(self): #free energy obtainable between attr,belief\n",
        "        #Input\n",
        "        input = Variable(self.mental_x.view(-1,1,1,1), requires_grad=True)\n",
        "            \n",
        "        #Generate mental/belief image\n",
        "        self.mental_img = self.network.decoder(input)\n",
        "    \n",
        "        #Error_attr \n",
        "        error_mental = (self.attractor_img - self.mental_img)\n",
        "        err = error_mental.detach()\n",
        "        self.error_mental.append(torch.mean(err)*1600) #The *1600 as it is a 40x40 img\n",
        "\n",
        "        dg_dmu_attr=(1/self.sigma_dyn)* error_mental \n",
        "\n",
        "        ## Backward pass\n",
        "        input.grad = torch.zeros(input.size())\n",
        "        self.mental_img.backward(dg_dmu_attr , retain_graph=True)\n",
        "        return input.grad\n",
        "\n",
        "    ## PROPRIOCEPTION\n",
        "    def proprioception(self):\n",
        "        #add noise to visual location (as this is a brain process and thus can contain some perceptual errors)\n",
        "        self.proprioception_loc_noise = self.AI_helper.add_noise(self.proprioception_loc, 0)\n",
        "        error_prop = (self.proprioception_loc_noise - self.mental_x)\n",
        "        self.error_prop.append(error_prop)\n",
        "        precision_prop = (1/self.sigma_prop) * error_prop\n",
        "        return precision_prop\n",
        "\n",
        "    def dynamical_prop(self): #free energy obtainable between attr,belief\n",
        "        #add noise to proprioceptive location (as this is a body process and thus can contain some perceptual errors)\n",
        "        mental_prop = self.AI_helper.add_noise(self.mental_x, 0)\n",
        "        error_mental_prop = (self.attractor_x - mental_prop)\n",
        "        self.error_mental_prop.append(error_mental_prop)\n",
        "        precision_mental_prop = (1/self.sigma_dyn_prop) * error_mental_prop\n",
        "        return precision_mental_prop\n",
        "    \n",
        "\n",
        "    '''\n",
        "    Info: Active inference with the capability of moving towards a goal\n",
        "    Input: based on initialization\n",
        "      -perception (optional):True/False for using perception to minimize variational free energy\n",
        "      -action (optional): True/False for using action to minimize variational free energy\n",
        "      -sense_vis (optional): True/False for using sensory vision\n",
        "      -sense_prop (optional): True/False for using sensory proprioception \n",
        "      -goal (optional): True/False for moving towards a goal state\n",
        "      -name (optional): Name of the resulting .gif\n",
        "    Output: -- a name.gif containing the progress --\n",
        "    '''\n",
        "    def active_inference(self, perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'armAI'):\n",
        "      fin = 0 #Used to reduce the number of itterations (based on mu_dot)\n",
        "      \n",
        "      for i in range(self.max_itt):\n",
        "          #Reset variables and save current state\n",
        "          save_state_v4(self, i, sense_vis, sense_prop, goal)\n",
        "          mu_vis = 0\n",
        "          mu_prop = 0\n",
        "          mu_action_vis = 0\n",
        "          mu_action_prop = 0\n",
        "          mu_goal_vis = 0 #preferred state\n",
        "          mu_goal_prop = 0 #preferred state\n",
        "\n",
        "          #Perception part of minimizing surprise\n",
        "          if perception:\n",
        "            if sense_vis:\n",
        "              mu_vis = self.visual()\n",
        "            if sense_prop:\n",
        "              self.proprioception_loc_noise = self.AI_helper.add_noise(self.proprioception_loc, 0)\n",
        "              mu_prop = self.prop_error(self.proprioception_loc_noise)\n",
        "          #Action part of minimizing surprise\n",
        "          if action:\n",
        "            if goal: #This would be an inferred action that would reduce future free energy (expected free energy [https://arxiv.org/pdf/2004.08128.pdf])\n",
        "              #goal is not a sensory state, thus needs to be generated trough the neural network to compare with mental state\n",
        "              mu_goal_vis =  self.visual_error(self.goal_vis)\n",
        "              mu_goal_prop =  self.prop_error(self.goal_x)\n",
        "\n",
        "            #Variational free energy\n",
        "            if sense_vis:\n",
        "                #error sensory to mental (-1 * error mental to sensory)\n",
        "                mu_action_vis = (-1) * self.visual_error(self.visual_img) \n",
        "            if sense_prop:\n",
        "                #error proprioception to mental (-1 * error mental to sensory)\n",
        "                mu_action_prop = (-1) *self.prop_error(self.proprioception_loc_noise)\n",
        "\n",
        "          #Sum the prediction errors\n",
        "          a_dot = self.action_gain*(  mu_action_vis + mu_action_prop) #action 'velocity'\n",
        "          mu_dot = self.perception_gain * (mu_vis + mu_prop) + self.goal_gain * (mu_goal_vis + mu_goal_prop) #mental 'velocity' (mu_goal_vis/prop = 0 if goal=False)\n",
        "          #print('mu_vis: {}\\n mu_prop: {}\\n mu_action_vis: {}\\n mu_action_prop: {}\\n mu_goal_vis: {}\\nmu_goal_prop: {}\\n\\n mu_dot: {}\\na_dot: {}\\n\\n'.format(mu_vis,mu_prop,mu_action_vis,mu_action_prop,mu_goal_vis,mu_goal_prop,mu_dot,a_dot))\n",
        "\n",
        "          #save data (plotting purposes)\n",
        "          if action:\n",
        "            self.a_dot_data.append(a_dot)\n",
        "          self.mu_dot_data.append(mu_dot)\n",
        "\n",
        "          #Update states\n",
        "          \n",
        "          new_vis_loc = torch.add(self.visual_x, a_dot, alpha=self.dt)\n",
        "          self.AI_helper.update_visual_state(new_vis_loc) #Update visual arm\n",
        "          self.proprioception_loc = torch.add(self.proprioception_loc,  a_dot, alpha=self.dt) #Update proprioceptive location\n",
        "          self.mental_x = torch.add(self.mental_x,  mu_dot, alpha=self.dt) #Update mental location\n",
        "          plt.close('all')\n",
        "\n",
        "          if self.adapt_sigma_vis and i > 0:\n",
        "              if np.square(self.pred_err_vis_prior).mean() < np.square(self.pred_err_vis).mean() and self.sigma_vis > 0.00: \n",
        "                self.sigma_vis = self.sigma_vis - 0.01\n",
        "                print('new sigma: {}\\n\\n'.format(self.sigma_vis))\n",
        "              else:\n",
        "                self.sigma_vis += 0.01\n",
        "          self.pred_err_vis_prior = self.pred_err_vis\n",
        "          #print('pred error: {}\\n\\n'.format(np.square(self.pred_err_vis).mean()))\n",
        "          #if self.adapt_sigma_vis and np.square(self.pred_err_vis).mean() <= 0.0356:\n",
        "            #self.sigma_vis = 0.08\n",
        "            #self.adapt_sigma_vis = False # Increase Sigma only once!\n",
        "            #print('update... sigma_vis: {}'.format(self.sigma_vis))\n",
        "          #Terminate if the agent's mental state has not moved (reducing runtime)\n",
        "          if abs(mu_dot) <= 0.05:\n",
        "            if fin == 15:\n",
        "              break\n",
        "            fin+= 1\n",
        "          else:\n",
        "            fin = 0\n",
        "          \n",
        "      #save all states to a gif\n",
        "      self.AI_helper.save_gif(name, action)\n",
        "    \n",
        "              \n",
        "            \n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcHkHmzPz5dy"
      },
      "source": [
        "The cell below contains test setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RBPcy_Mz4ip",
        "outputId": "d72f910c-1a75-4a23-e1d0-e49d3cbf706c"
      },
      "source": [
        "\n",
        "##TEST##\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.1, noise = False)  #Create visual state\n",
        "mental_loc = -0.1 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "\n",
        "armAI = ArmAI(max_itt = 20)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = False, sense_vis = True, sense_prop = True, goal = False, name = 'test1')\n",
        "\n",
        "#Active inference, correctly initialized (test if nothing strange happens) | agent should not move\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0, noise = False)  #Create visual state\n",
        "mental_loc = 0 # Does not require an image\n",
        "proprioception_loc = 0\n",
        "\n",
        "armAI = ArmAI(max_itt = 50)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'armAI_BaseCase')\n",
        "\n",
        "\n",
        "\n",
        "#Perception only (predictive processing), prop & vis No overlap\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = 0.3\n",
        "\n",
        "armAI = ArmAI(max_itt = 20)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = False, sense_vis = True, sense_prop = True, goal = False, name = 'armAI_percep_NoOverlap')\n",
        "\n",
        "#Perception only (predictive processing), prop & vis with overlap\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = -0.3\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = False, sense_vis = True, sense_prop = True, goal = False, name = 'armAI_percep_Overlap')\n",
        "\n",
        "#active inference, prop & vis No overlap\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location =0.2, noise = False)  #Create visual state\n",
        "mental_loc = 0.2 # Does not require an image\n",
        "proprioception_loc = -0.6\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'armAI_AI_NoOverlap')\n",
        "\n",
        "#Active inference, prop & vis with overlap\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.7, noise = False)  #Create visual state\n",
        "mental_loc = 0.7 # Does not require an image\n",
        "proprioception_loc = 0.3\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'armAI_AI_Overlap')\n",
        "\n",
        "#Active inference, with goal (EFE) |vis and prop correct\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.7, noise = False)  #Create visual state\n",
        "goal_img, goal_loc = create_tensor_v2('init goal', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = 0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = True, name = 'armAI_AI_goal_v2')\n",
        "\n",
        "#Active inference, with goal (EFE) |All random init\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.9, noise = False)  #Create visual state\n",
        "goal_img, goal_loc = create_tensor_v2('init goal', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = 0.4 # Does not require an image\n",
        "proprioception_loc = -0.1\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = True, name = 'armAI_AI_goal_randomInit')\n",
        "\n",
        "#--------- Special cases ----------\n",
        "#Active inference, Proprioception only \n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.9, noise = False)  #Create visual state\n",
        "goal_img, goal_loc = create_tensor_v2('init goal', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = 0.9 # Does not require an image\n",
        "proprioception_loc = -0.1\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = False, sense_prop = True, goal = True, name = 'armAI_AI_PropOnly_goal')\n",
        "\n",
        "#Active inference, Visual only \n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.9, noise = False)  #Create visual state\n",
        "goal_img, goal_loc = create_tensor_v2('init goal', location = 0.2, noise = False)  #Create visual state\n",
        "mental_loc = -0.9 # Does not require an image\n",
        "proprioception_loc = -0.9\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 100)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = False, goal = True, name = 'armAI_AI_visOnly_goal')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new sigma: 0.14\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0157]]]])\n",
            " mu_prop: tensor([[[[4.6467]]]])\n",
            " mu_action_vis: tensor([[[[-0.0219]]]])\n",
            " mu_action_prop: tensor([[[[-4.6467]]]])\n",
            " mu_goal_vis: tensor([[[[-6.3345]]]])\n",
            "mu_goal_prop: tensor([[[[-3.1387]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2405]]]])\n",
            "a_dot: tensor([[[[-0.2334]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0600]]]])\n",
            " mu_prop: tensor([[[[4.6609]]]])\n",
            " mu_action_vis: tensor([[[[-0.0900]]]])\n",
            " mu_action_prop: tensor([[[[-4.6609]]]])\n",
            " mu_goal_vis: tensor([[[[-5.9023]]]])\n",
            "mu_goal_prop: tensor([[[[-2.6576]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1920]]]])\n",
            "a_dot: tensor([[[[-0.2375]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.0912]]]])\n",
            " mu_prop: tensor([[[[4.5697]]]])\n",
            " mu_action_vis: tensor([[[[0.1458]]]])\n",
            " mu_action_prop: tensor([[[[-4.5697]]]])\n",
            " mu_goal_vis: tensor([[[[-5.5421]]]])\n",
            "mu_goal_prop: tensor([[[[-2.2737]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1669]]]])\n",
            "a_dot: tensor([[[[-0.2212]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.15000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.0600]]]])\n",
            " mu_prop: tensor([[[[4.4611]]]])\n",
            " mu_action_vis: tensor([[[[0.0900]]]])\n",
            " mu_action_prop: tensor([[[[-4.4611]]]])\n",
            " mu_goal_vis: tensor([[[[-4.2417]]]])\n",
            "mu_goal_prop: tensor([[[[-1.9400]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0890]]]])\n",
            "a_dot: tensor([[[[-0.2186]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1771]]]])\n",
            " mu_prop: tensor([[[[4.2020]]]])\n",
            " mu_action_vis: tensor([[[[-0.2834]]]])\n",
            " mu_action_prop: tensor([[[[-4.2020]]]])\n",
            " mu_goal_vis: tensor([[[[-2.9481]]]])\n",
            "mu_goal_prop: tensor([[[[-1.7619]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0165]]]])\n",
            "a_dot: tensor([[[[-0.2243]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1497]]]])\n",
            " mu_prop: tensor([[[[3.7866]]]])\n",
            " mu_action_vis: tensor([[[[-0.2545]]]])\n",
            " mu_action_prop: tensor([[[[-3.7866]]]])\n",
            " mu_goal_vis: tensor([[[[-2.9479]]]])\n",
            "mu_goal_prop: tensor([[[[-1.7289]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0370]]]])\n",
            "a_dot: tensor([[[[-0.2021]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.16000000000000003\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1702]]]])\n",
            " mu_prop: tensor([[[[3.4565]]]])\n",
            " mu_action_vis: tensor([[[[-0.2723]]]])\n",
            " mu_action_prop: tensor([[[[-3.4565]]]])\n",
            " mu_goal_vis: tensor([[[[-4.8265]]]])\n",
            "mu_goal_prop: tensor([[[[-1.6548]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1427]]]])\n",
            "a_dot: tensor([[[[-0.1864]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0909]]]])\n",
            " mu_prop: tensor([[[[3.3691]]]])\n",
            " mu_action_vis: tensor([[[[-0.1546]]]])\n",
            " mu_action_prop: tensor([[[[-3.3691]]]])\n",
            " mu_goal_vis: tensor([[[[-3.2002]]]])\n",
            "mu_goal_prop: tensor([[[[-1.3693]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0555]]]])\n",
            "a_dot: tensor([[[[-0.1762]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0714]]]])\n",
            " mu_prop: tensor([[[[3.1277]]]])\n",
            " mu_action_vis: tensor([[[[-0.1286]]]])\n",
            " mu_action_prop: tensor([[[[-3.1277]]]])\n",
            " mu_goal_vis: tensor([[[[-2.7951]]]])\n",
            "mu_goal_prop: tensor([[[[-1.2584]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0427]]]])\n",
            "a_dot: tensor([[[[-0.1628]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0746]]]])\n",
            " mu_prop: tensor([[[[2.8875]]]])\n",
            " mu_action_vis: tensor([[[[-0.1417]]]])\n",
            " mu_action_prop: tensor([[[[-2.8875]]]])\n",
            " mu_goal_vis: tensor([[[[-2.4391]]]])\n",
            "mu_goal_prop: tensor([[[[-1.1730]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0325]]]])\n",
            "a_dot: tensor([[[[-0.1515]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.18000000000000005\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0755]]]])\n",
            " mu_prop: tensor([[[[2.6496]]]])\n",
            " mu_action_vis: tensor([[[[-0.1358]]]])\n",
            " mu_action_prop: tensor([[[[-2.6496]]]])\n",
            " mu_goal_vis: tensor([[[[-2.3626]]]])\n",
            "mu_goal_prop: tensor([[[[-1.1080]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0373]]]])\n",
            "a_dot: tensor([[[[-0.1393]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0483]]]])\n",
            " mu_prop: tensor([[[[2.4456]]]])\n",
            " mu_action_vis: tensor([[[[-0.0918]]]])\n",
            " mu_action_prop: tensor([[[[-2.4456]]]])\n",
            " mu_goal_vis: tensor([[[[-1.7455]]]])\n",
            "mu_goal_prop: tensor([[[[-1.0334]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0143]]]])\n",
            "a_dot: tensor([[[[-0.1269]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0491]]]])\n",
            " mu_prop: tensor([[[[2.2203]]]])\n",
            " mu_action_vis: tensor([[[[-0.0982]]]])\n",
            " mu_action_prop: tensor([[[[-2.2203]]]])\n",
            " mu_goal_vis: tensor([[[[-1.8377]]]])\n",
            "mu_goal_prop: tensor([[[[-1.0049]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0287]]]])\n",
            "a_dot: tensor([[[[-0.1159]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0197]]]])\n",
            " mu_prop: tensor([[[[2.0458]]]])\n",
            " mu_action_vis: tensor([[[[-0.0414]]]])\n",
            " mu_action_prop: tensor([[[[-2.0458]]]])\n",
            " mu_goal_vis: tensor([[[[-1.9292]]]])\n",
            "mu_goal_prop: tensor([[[[-0.9476]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0406]]]])\n",
            "a_dot: tensor([[[[-0.1044]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.20000000000000007\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0296]]]])\n",
            " mu_prop: tensor([[[[1.9182]]]])\n",
            " mu_action_vis: tensor([[[[-0.0592]]]])\n",
            " mu_action_prop: tensor([[[[-1.9182]]]])\n",
            " mu_goal_vis: tensor([[[[-1.9334]]]])\n",
            "mu_goal_prop: tensor([[[[-0.8665]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0426]]]])\n",
            "a_dot: tensor([[[[-0.0989]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.19000000000000006\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0364]]]])\n",
            " mu_prop: tensor([[[[1.8057]]]])\n",
            " mu_action_vis: tensor([[[[-0.0692]]]])\n",
            " mu_action_prop: tensor([[[[-1.8057]]]])\n",
            " mu_goal_vis: tensor([[[[-1.6550]]]])\n",
            "mu_goal_prop: tensor([[[[-0.7812]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0297]]]])\n",
            "a_dot: tensor([[[[-0.0937]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0312]]]])\n",
            " mu_prop: tensor([[[[1.6776]]]])\n",
            " mu_action_vis: tensor([[[[-0.0624]]]])\n",
            " mu_action_prop: tensor([[[[-1.6776]]]])\n",
            " mu_goal_vis: tensor([[[[-1.6380]]]])\n",
            "mu_goal_prop: tensor([[[[-0.7218]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0326]]]])\n",
            "a_dot: tensor([[[[-0.0870]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0387]]]])\n",
            " mu_prop: tensor([[[[1.5687]]]])\n",
            " mu_action_vis: tensor([[[[-0.0814]]]])\n",
            " mu_action_prop: tensor([[[[-1.5687]]]])\n",
            " mu_goal_vis: tensor([[[[-1.6463]]]])\n",
            "mu_goal_prop: tensor([[[[-0.6567]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0348]]]])\n",
            "a_dot: tensor([[[[-0.0825]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0378]]]])\n",
            " mu_prop: tensor([[[[1.4732]]]])\n",
            " mu_action_vis: tensor([[[[-0.0832]]]])\n",
            " mu_action_prop: tensor([[[[-1.4732]]]])\n",
            " mu_goal_vis: tensor([[[[-1.2932]]]])\n",
            "mu_goal_prop: tensor([[[[-0.5872]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0185]]]])\n",
            "a_dot: tensor([[[[-0.0778]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.21000000000000008\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0453]]]])\n",
            " mu_prop: tensor([[[[1.3545]]]])\n",
            " mu_action_vis: tensor([[[[-0.0950]]]])\n",
            " mu_action_prop: tensor([[[[-1.3545]]]])\n",
            " mu_goal_vis: tensor([[[[-1.3982]]]])\n",
            "mu_goal_prop: tensor([[[[-0.5503]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0274]]]])\n",
            "a_dot: tensor([[[[-0.0725]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0464]]]])\n",
            " mu_prop: tensor([[[[1.2644]]]])\n",
            " mu_action_vis: tensor([[[[-0.1021]]]])\n",
            " mu_action_prop: tensor([[[[-1.2644]]]])\n",
            " mu_goal_vis: tensor([[[[-1.4095]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4954]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0297]]]])\n",
            "a_dot: tensor([[[[-0.0683]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.21000000000000008\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0549]]]])\n",
            " mu_prop: tensor([[[[1.1872]]]])\n",
            " mu_action_vis: tensor([[[[-0.1153]]]])\n",
            " mu_action_prop: tensor([[[[-1.1872]]]])\n",
            " mu_goal_vis: tensor([[[[-1.1896]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4360]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0192]]]])\n",
            "a_dot: tensor([[[[-0.0651]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.20000000000000007\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0627]]]])\n",
            " mu_prop: tensor([[[[1.0953]]]])\n",
            " mu_action_vis: tensor([[[[-0.1253]]]])\n",
            " mu_action_prop: tensor([[[[-1.0953]]]])\n",
            " mu_goal_vis: tensor([[[[-0.5292]]]])\n",
            "mu_goal_prop: tensor([[[[-0.3976]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0116]]]])\n",
            "a_dot: tensor([[[[-0.0610]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.19000000000000006\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0698]]]])\n",
            " mu_prop: tensor([[[[0.9501]]]])\n",
            " mu_action_vis: tensor([[[[-0.1327]]]])\n",
            " mu_action_prop: tensor([[[[-0.9501]]]])\n",
            " mu_goal_vis: tensor([[[[-0.5294]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4207]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0035]]]])\n",
            "a_dot: tensor([[[[-0.0541]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.18000000000000005\n",
            "\n",
            "\n",
            "No neural network was given. Searching in folder networks for 'trained_network_DataLarge_Hybrid.pth'\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([-20.])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([20.])\n",
            " mu_goal_vis: tensor([[[[-0.0971]]]])\n",
            "mu_goal_prop: tensor([-32.0000])\n",
            "\n",
            " mu_dot: tensor([[[[-2.6049]]]])\n",
            "a_dot: tensor([1.])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[-12.7903]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[12.7903]]]])\n",
            " mu_goal_vis: tensor([[[[0.0632]]]])\n",
            "mu_goal_prop: tensor([[[[-26.7903]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-1.9759]]]])\n",
            "a_dot: tensor([[[[0.6395]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[-7.5595]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[7.5595]]]])\n",
            " mu_goal_vis: tensor([[[[0.1094]]]])\n",
            "mu_goal_prop: tensor([[[[-22.8386]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-1.5144]]]])\n",
            "a_dot: tensor([[[[0.3780]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[-3.7747]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[3.7747]]]])\n",
            " mu_goal_vis: tensor([[[[0.1885]]]])\n",
            "mu_goal_prop: tensor([[[[-19.8097]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-1.1698]]]])\n",
            "a_dot: tensor([[[[0.1887]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[-1.0576]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[1.0576]]]])\n",
            " mu_goal_vis: tensor([[[[0.1454]]]])\n",
            "mu_goal_prop: tensor([[[[-17.4701]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.9191]]]])\n",
            "a_dot: tensor([[[[0.0529]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[0.8864]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-0.8864]]]])\n",
            " mu_goal_vis: tensor([[[[-0.6762]]]])\n",
            "mu_goal_prop: tensor([[[[-15.6319]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.7711]]]])\n",
            "a_dot: tensor([[[[-0.0443]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.3399]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.3399]]]])\n",
            " mu_goal_vis: tensor([[[[-0.1681]]]])\n",
            "mu_goal_prop: tensor([[[[-14.0897]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.5959]]]])\n",
            "a_dot: tensor([[[[-0.1170]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.2977]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.2977]]]])\n",
            " mu_goal_vis: tensor([[[[-0.5989]]]])\n",
            "mu_goal_prop: tensor([[[[-12.8979]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.5100]]]])\n",
            "a_dot: tensor([[[[-0.1649]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.9878]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.9878]]]])\n",
            " mu_goal_vis: tensor([[[[-0.2022]]]])\n",
            "mu_goal_prop: tensor([[[[-11.8780]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.4046]]]])\n",
            "a_dot: tensor([[[[-0.1994]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.3983]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.3983]]]])\n",
            " mu_goal_vis: tensor([[[[-1.7615]]]])\n",
            "mu_goal_prop: tensor([[[[-11.0688]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.4216]]]])\n",
            "a_dot: tensor([[[[-0.2199]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.8017]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.8017]]]])\n",
            " mu_goal_vis: tensor([[[[-0.8515]]]])\n",
            "mu_goal_prop: tensor([[[[-10.2256]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.3138]]]])\n",
            "a_dot: tensor([[[[-0.2401]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.9490]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.9490]]]])\n",
            " mu_goal_vis: tensor([[[[-0.6773]]]])\n",
            "mu_goal_prop: tensor([[[[-9.5980]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2663]]]])\n",
            "a_dot: tensor([[[[-0.2475]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.9868]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.9868]]]])\n",
            " mu_goal_vis: tensor([[[[-0.5067]]]])\n",
            "mu_goal_prop: tensor([[[[-9.0654]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2293]]]])\n",
            "a_dot: tensor([[[[-0.2493]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.9466]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.9466]]]])\n",
            " mu_goal_vis: tensor([[[[-2.2455]]]])\n",
            "mu_goal_prop: tensor([[[[-8.6069]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2953]]]])\n",
            "a_dot: tensor([[[[-0.2473]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.0425]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.0425]]]])\n",
            " mu_goal_vis: tensor([[[[-5.1311]]]])\n",
            "mu_goal_prop: tensor([[[[-8.0163]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.4052]]]])\n",
            "a_dot: tensor([[[[-0.2521]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.3488]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.3488]]]])\n",
            " mu_goal_vis: tensor([[[[-4.1800]]]])\n",
            "mu_goal_prop: tensor([[[[-7.2058]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.3019]]]])\n",
            "a_dot: tensor([[[[-0.2674]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.4176]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.4176]]]])\n",
            " mu_goal_vis: tensor([[[[-3.9447]]]])\n",
            "mu_goal_prop: tensor([[[[-6.6021]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2565]]]])\n",
            "a_dot: tensor([[[[-0.2709]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.3888]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.3888]]]])\n",
            " mu_goal_vis: tensor([[[[-3.6932]]]])\n",
            "mu_goal_prop: tensor([[[[-6.0892]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2197]]]])\n",
            "a_dot: tensor([[[[-0.2694]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.2892]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.2892]]]])\n",
            " mu_goal_vis: tensor([[[[-3.5599]]]])\n",
            "mu_goal_prop: tensor([[[[-5.6498]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1960]]]])\n",
            "a_dot: tensor([[[[-0.2645]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[5.1524]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-5.1524]]]])\n",
            " mu_goal_vis: tensor([[[[-3.3849]]]])\n",
            "mu_goal_prop: tensor([[[[-5.2578]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1745]]]])\n",
            "a_dot: tensor([[[[-0.2576]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.9862]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.9862]]]])\n",
            " mu_goal_vis: tensor([[[[-3.1750]]]])\n",
            "mu_goal_prop: tensor([[[[-4.9087]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1549]]]])\n",
            "a_dot: tensor([[[[-0.2493]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.7973]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.7973]]]])\n",
            " mu_goal_vis: tensor([[[[-5.6031]]]])\n",
            "mu_goal_prop: tensor([[[[-4.5990]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2702]]]])\n",
            "a_dot: tensor([[[[-0.2399]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.8581]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.8581]]]])\n",
            " mu_goal_vis: tensor([[[[-5.4387]]]])\n",
            "mu_goal_prop: tensor([[[[-4.0585]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2320]]]])\n",
            "a_dot: tensor([[[[-0.2429]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.8362]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.8362]]]])\n",
            " mu_goal_vis: tensor([[[[-5.5723]]]])\n",
            "mu_goal_prop: tensor([[[[-3.5946]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2165]]]])\n",
            "a_dot: tensor([[[[-0.2418]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.7856]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.7856]]]])\n",
            " mu_goal_vis: tensor([[[[-6.3440]]]])\n",
            "mu_goal_prop: tensor([[[[-3.1615]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.2360]]]])\n",
            "a_dot: tensor([[[[-0.2393]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.7790]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.7790]]]])\n",
            " mu_goal_vis: tensor([[[[-5.9784]]]])\n",
            "mu_goal_prop: tensor([[[[-2.6895]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1944]]]])\n",
            "a_dot: tensor([[[[-0.2390]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.6900]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.6900]]]])\n",
            " mu_goal_vis: tensor([[[[-5.5424]]]])\n",
            "mu_goal_prop: tensor([[[[-2.3006]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1577]]]])\n",
            "a_dot: tensor([[[[-0.2345]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.5363]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.5363]]]])\n",
            " mu_goal_vis: tensor([[[[-4.2687]]]])\n",
            "mu_goal_prop: tensor([[[[-1.9853]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0859]]]])\n",
            "a_dot: tensor([[[[-0.2268]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[4.2545]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-4.2545]]]])\n",
            " mu_goal_vis: tensor([[[[-2.9484]]]])\n",
            "mu_goal_prop: tensor([[[[-1.8136]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0254]]]])\n",
            "a_dot: tensor([[[[-0.2127]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.8798]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.8798]]]])\n",
            " mu_goal_vis: tensor([[[[-2.9481]]]])\n",
            "mu_goal_prop: tensor([[[[-1.7628]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0416]]]])\n",
            "a_dot: tensor([[[[-0.1940]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.5749]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.5749]]]])\n",
            " mu_goal_vis: tensor([[[[-4.8269]]]])\n",
            "mu_goal_prop: tensor([[[[-1.6797]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.1466]]]])\n",
            "a_dot: tensor([[[[-0.1787]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.5106]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.5106]]]])\n",
            " mu_goal_vis: tensor([[[[-3.4506]]]])\n",
            "mu_goal_prop: tensor([[[[-1.3865]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0663]]]])\n",
            "a_dot: tensor([[[[-0.1755]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.2922]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.2922]]]])\n",
            " mu_goal_vis: tensor([[[[-2.7951]]]])\n",
            "mu_goal_prop: tensor([[[[-1.2539]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0378]]]])\n",
            "a_dot: tensor([[[[-0.1646]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[3.0386]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-3.0386]]]])\n",
            " mu_goal_vis: tensor([[[[-2.4037]]]])\n",
            "mu_goal_prop: tensor([[[[-1.1782]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0272]]]])\n",
            "a_dot: tensor([[[[-0.1519]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.7891]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.7891]]]])\n",
            " mu_goal_vis: tensor([[[[-2.2067]]]])\n",
            "mu_goal_prop: tensor([[[[-1.1239]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0271]]]])\n",
            "a_dot: tensor([[[[-0.1395]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.5643]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.5643]]]])\n",
            " mu_goal_vis: tensor([[[[-2.2410]]]])\n",
            "mu_goal_prop: tensor([[[[-1.0697]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0373]]]])\n",
            "a_dot: tensor([[[[-0.1282]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.3825]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.3825]]]])\n",
            " mu_goal_vis: tensor([[[[-2.0302]]]])\n",
            "mu_goal_prop: tensor([[[[-0.9951]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0321]]]])\n",
            "a_dot: tensor([[[[-0.1191]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.2086]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.2086]]]])\n",
            " mu_goal_vis: tensor([[[[-1.9103]]]])\n",
            "mu_goal_prop: tensor([[[[-0.9308]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0316]]]])\n",
            "a_dot: tensor([[[[-0.1104]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[2.0510]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-2.0510]]]])\n",
            " mu_goal_vis: tensor([[[[-1.9335]]]])\n",
            "mu_goal_prop: tensor([[[[-0.8675]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0375]]]])\n",
            "a_dot: tensor([[[[-0.1025]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.9209]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.9209]]]])\n",
            " mu_goal_vis: tensor([[[[-1.6551]]]])\n",
            "mu_goal_prop: tensor([[[[-0.7925]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0263]]]])\n",
            "a_dot: tensor([[[[-0.0960]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.7815]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.7815]]]])\n",
            " mu_goal_vis: tensor([[[[-1.5959]]]])\n",
            "mu_goal_prop: tensor([[[[-0.7399]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0277]]]])\n",
            "a_dot: tensor([[[[-0.0891]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.6587]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.6587]]]])\n",
            " mu_goal_vis: tensor([[[[-1.6377]]]])\n",
            "mu_goal_prop: tensor([[[[-0.6844]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0332]]]])\n",
            "a_dot: tensor([[[[-0.0829]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.5592]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.5592]]]])\n",
            " mu_goal_vis: tensor([[[[-1.5841]]]])\n",
            "mu_goal_prop: tensor([[[[-0.6181]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0321]]]])\n",
            "a_dot: tensor([[[[-0.0780]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.4676]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.4676]]]])\n",
            " mu_goal_vis: tensor([[[[-1.3982]]]])\n",
            "mu_goal_prop: tensor([[[[-0.5538]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0242]]]])\n",
            "a_dot: tensor([[[[-0.0734]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.3693]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.3693]]]])\n",
            " mu_goal_vis: tensor([[[[-1.4096]]]])\n",
            "mu_goal_prop: tensor([[[[-0.5054]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0273]]]])\n",
            "a_dot: tensor([[[[-0.0685]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.2869]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.2869]]]])\n",
            " mu_goal_vis: tensor([[[[-1.1727]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4508]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0168]]]])\n",
            "a_dot: tensor([[[[-0.0643]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.1919]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.1919]]]])\n",
            " mu_goal_vis: tensor([[[[-0.5293]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4171]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0123]]]])\n",
            "a_dot: tensor([[[[-0.0596]]]])\n",
            "\n",
            "\n",
            "mu_vis: 0\n",
            " mu_prop: tensor([[[[1.0482]]]])\n",
            " mu_action_vis: 0\n",
            " mu_action_prop: tensor([[[[-1.0482]]]])\n",
            " mu_goal_vis: tensor([[[[-1.1897]]]])\n",
            "mu_goal_prop: tensor([[[[-0.4417]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0292]]]])\n",
            "a_dot: tensor([[[[-0.0524]]]])\n",
            "\n",
            "\n",
            "No neural network was given. Searching in folder networks for 'trained_network_DataLarge_Hybrid.pth'\n",
            "mu_vis: tensor([[[[-1.0308]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[1.0308]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3300]]]])\n",
            "mu_goal_prop: tensor([22.])\n",
            "\n",
            " mu_dot: tensor([[[[1.0320]]]])\n",
            "a_dot: tensor([[[[0.0515]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-3.0796]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[3.0796]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2946]]]])\n",
            "mu_goal_prop: tensor([[[[19.9361]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.8281]]]])\n",
            "a_dot: tensor([[[[0.1540]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-4.5876]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[5.0464]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.0853]]]])\n",
            "mu_goal_prop: tensor([[[[18.2799]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.6803]]]])\n",
            "a_dot: tensor([[[[0.2523]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-4.4157]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[5.2989]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.3850]]]])\n",
            "mu_goal_prop: tensor([[[[16.9192]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.6444]]]])\n",
            "a_dot: tensor([[[[0.2649]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.11\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-5.0473]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[5.5520]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.0981]]]])\n",
            "mu_goal_prop: tensor([[[[15.6303]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.5242]]]])\n",
            "a_dot: tensor([[[[0.2776]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-4.1444]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[4.9733]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.0014]]]])\n",
            "mu_goal_prop: tensor([[[[14.5818]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.5218]]]])\n",
            "a_dot: tensor([[[[0.2487]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.11\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-4.4646]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[4.9111]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-1.0646]]]])\n",
            "mu_goal_prop: tensor([[[[13.5382]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4005]]]])\n",
            "a_dot: tensor([[[[0.2456]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.1\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-2.7110]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[2.7110]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.9010]]]])\n",
            "mu_goal_prop: tensor([[[[12.7373]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.5464]]]])\n",
            "a_dot: tensor([[[[0.1356]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-3.4385]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[3.7824]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.1514]]]])\n",
            "mu_goal_prop: tensor([[[[11.6446]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4179]]]])\n",
            "a_dot: tensor([[[[0.1891]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.1\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-3.8472]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[3.8472]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.5949]]]])\n",
            "mu_goal_prop: tensor([[[[10.8089]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.3778]]]])\n",
            "a_dot: tensor([[[[0.1924]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.09000000000000001\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-5.3306]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[4.7975]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.7297]]]])\n",
            "mu_goal_prop: tensor([[[[10.0532]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.2726]]]])\n",
            "a_dot: tensor([[[[0.2399]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-2.7236]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[2.7236]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[1.2029]]]])\n",
            "mu_goal_prop: tensor([[[[9.5080]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.3994]]]])\n",
            "a_dot: tensor([[[[0.1362]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-1.0449]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[1.1494]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[2.3377]]]])\n",
            "mu_goal_prop: tensor([[[[8.7093]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.5001]]]])\n",
            "a_dot: tensor([[[[0.0575]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.1\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-1.3917]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[1.3917]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[2.8369]]]])\n",
            "mu_goal_prop: tensor([[[[7.7091]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4577]]]])\n",
            "a_dot: tensor([[[[0.0696]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.09000000000000001\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.4667]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.4201]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[3.7923]]]])\n",
            "mu_goal_prop: tensor([[[[6.7936]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.5526]]]])\n",
            "a_dot: tensor([[[[-0.0210]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.6396]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[0.6396]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[4.6764]]]])\n",
            "mu_goal_prop: tensor([[[[5.6884]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4863]]]])\n",
            "a_dot: tensor([[[[0.0320]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.09000000000000001\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-1.2521]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[1.1269]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[5.3412]]]])\n",
            "mu_goal_prop: tensor([[[[4.7159]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4402]]]])\n",
            "a_dot: tensor([[[[0.0563]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.4087]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[0.4087]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[5.6143]]]])\n",
            "mu_goal_prop: tensor([[[[3.8354]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4520]]]])\n",
            "a_dot: tensor([[[[0.0204]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.09000000000000001\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.1542]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[0.1388]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[6.2908]]]])\n",
            "mu_goal_prop: tensor([[[[2.9313]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.4534]]]])\n",
            "a_dot: tensor([[[[0.0069]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.08000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[-0.8915]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[0.7132]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[5.3685]]]])\n",
            "mu_goal_prop: tensor([[[[2.0245]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.3251]]]])\n",
            "a_dot: tensor([[[[0.0357]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.07000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[1.4111]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.9878]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[1.4975]]]])\n",
            "mu_goal_prop: tensor([[[[1.3743]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.2141]]]])\n",
            "a_dot: tensor([[[[-0.0494]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.06000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[1.0966]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.6580]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.7448]]]])\n",
            "mu_goal_prop: tensor([[[[0.9460]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.1394]]]])\n",
            "a_dot: tensor([[[[-0.0329]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.6339]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.4437]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[0.3419]]]])\n",
            "mu_goal_prop: tensor([[[[0.6673]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0822]]]])\n",
            "a_dot: tensor([[[[-0.0222]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.4011]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.3208]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2441]]]])\n",
            "mu_goal_prop: tensor([[[[0.5030]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0330]]]])\n",
            "a_dot: tensor([[[[-0.0160]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.2201]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1981]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2913]]]])\n",
            "mu_goal_prop: tensor([[[[0.4370]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0183]]]])\n",
            "a_dot: tensor([[[[-0.0099]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1814]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1814]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2872]]]])\n",
            "mu_goal_prop: tensor([[[[0.4004]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0147]]]])\n",
            "a_dot: tensor([[[[-0.0091]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1082]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1191]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2985]]]])\n",
            "mu_goal_prop: tensor([[[[0.3710]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0090]]]])\n",
            "a_dot: tensor([[[[-0.0060]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.1\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1358]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1358]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.2998]]]])\n",
            "mu_goal_prop: tensor([[[[0.3529]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0094]]]])\n",
            "a_dot: tensor([[[[-0.0068]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1233]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1356]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3000]]]])\n",
            "mu_goal_prop: tensor([[[[0.3340]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0079]]]])\n",
            "a_dot: tensor([[[[-0.0068]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.1021]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1225]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3001]]]])\n",
            "mu_goal_prop: tensor([[[[0.3183]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0060]]]])\n",
            "a_dot: tensor([[[[-0.0061]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0774]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1006]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3880]]]])\n",
            "mu_goal_prop: tensor([[[[0.3062]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0002]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0718]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1006]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3880]]]])\n",
            "mu_goal_prop: tensor([[[[0.3067]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0005]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0670]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1006]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3880]]]])\n",
            "mu_goal_prop: tensor([[[[0.3076]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0007]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0627]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1003]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3094]]]])\n",
            "mu_goal_prop: tensor([[[[0.3090]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0031]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.15000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0670]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1005]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3881]]]])\n",
            "mu_goal_prop: tensor([[[[0.3027]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0009]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0628]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1005]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3880]]]])\n",
            "mu_goal_prop: tensor([[[[0.3046]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0010]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0592]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1006]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3880]]]])\n",
            "mu_goal_prop: tensor([[[[0.3066]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0011]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.16000000000000003\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0627]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1003]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3094]]]])\n",
            "mu_goal_prop: tensor([[[[0.3089]]]])\n",
            "\n",
            " mu_dot: tensor([[[[0.0031]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n",
            "new sigma: 0.15000000000000002\n",
            "\n",
            "\n",
            "mu_vis: tensor([[[[0.0670]]]])\n",
            " mu_prop: 0\n",
            " mu_action_vis: tensor([[[[-0.1005]]]])\n",
            " mu_action_prop: 0\n",
            " mu_goal_vis: tensor([[[[-0.3881]]]])\n",
            "mu_goal_prop: tensor([[[[0.3026]]]])\n",
            "\n",
            " mu_dot: tensor([[[[-0.0009]]]])\n",
            "a_dot: tensor([[[[-0.0050]]]])\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}