{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArmAIF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEvG+BDIpU7r5jeSVzXENg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rfrowein/armAIF/blob/main/ArmAIF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhRVYrwfC5OT"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import Rectangle, Circle\n",
        "from matplotlib.transforms import Bbox\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import imageio\n",
        "\n",
        "from statistics import mean\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import cv2\n",
        "from skimage.util import random_noise\n",
        "from PIL import Image\n",
        "\n",
        "#create folder /data in current working directory, if it does not exist yet\n",
        "if not os.path.exists(os.getcwd()+'/data'):\n",
        "    os.mkdir(os.getcwd()+'/data')\n",
        "\n",
        "\n",
        "#Create folder /networks in current working directory, if it does not exist yet\n",
        "if not os.path.exists(os.getcwd()+'/networks'):\n",
        "        os.mkdir(os.getcwd()+'/networks')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpEQ2OnrD8Rs"
      },
      "source": [
        "'''\n",
        "Info: Create an environment containing an arm\n",
        "\n",
        "Input:\n",
        "  -position: Location of the arm in the environment [-1,1]\n",
        "  -name: Name of the newly created figure (.png,.jpg, etc. is not needed)\n",
        "\n",
        "Output: -- save figure --\n",
        "'''\n",
        "def create_image_v2(position, name):\n",
        "\n",
        "    image = mpimg.imread('arm.png')\n",
        "    \n",
        "    #frameon = false, removes the outer axis --> needed as when turning axis('off') will also remove background\n",
        "    fig = plt.figure(figsize=(2,1.5),frameon=False)\n",
        "    \n",
        "    #Create environment, y:[0, 1.5] x:[0, 2.7] (the 2.7 is the -1,1 environment + 0.7 of arm width preventing out of bounds)\n",
        "    ax1 = fig.add_axes([0, 0, 2.7, 1.5])\n",
        "    \n",
        "    #Create arm, which is plotted inside the environment\n",
        "    ax2 = fig.add_axes([(position+1), 0, 0.7, 1]) #as the plot runs from 0 to 2 instead of -1 to 1, +1 is added\n",
        "    ax2.axis('off')\n",
        "    ax2.imshow(image, aspect='auto', zorder=-1) #add image\n",
        "    \n",
        "    #Variable extent is needed to prevent padding from forming when saving. \n",
        "      # -as the saved image is used for the neural network padding breaks it (when using a black background)\n",
        "    #Bbox_inches = 'tight' will leave padding\n",
        "    #Bbox_inches = 0 will leave no usable image\n",
        "    #Pad_inches = 0 will leave padding, setting it to -0.32 will remove padding, but deform image (altough still usable)\n",
        "    extent = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted()) \n",
        "    plt.savefig(name + '.png',bbox_inches=extent)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGepyMP_JTP6"
      },
      "source": [
        "'''\n",
        "Info: Create image, read image (add noise) and transform it to a tensor\n",
        "\n",
        "Input:\n",
        "  -name: String containing the name of the to be saved image (exclude .png)\n",
        "  -location (optional): Single x-axis positional input\n",
        "  -noise: True/False for adding Gaussian noise to the image (0.01)\n",
        "\n",
        "Outout: img_tensor , location\n",
        "  -img_tensor: Tensor of the image\n",
        "  -location: The x location of the image\n",
        "'''\n",
        "\n",
        "def create_tensor_v2(name, location = None, noise = True):\n",
        "    \n",
        "    if location is None:\n",
        "        location = round(random.uniform(-1, 1),2) # Create random location between -1 and 1 (2 decimal)\n",
        "        \n",
        "    create_image_v2(location,name) # Create image\n",
        "    \n",
        "    #Create and read image\n",
        "    img = cv2.imread(name + '.png',0)\n",
        "    img = cv2.resize(img, (40,40))\n",
        "    if noise:\n",
        "      img = random_noise(img,mode='gaussian')\n",
        "    \n",
        "    #Convert image to tensor\n",
        "    img_tensor = torch.from_numpy(img)\n",
        "    return img_tensor, location  #converts the nparray image to tensor (I,X)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdnTbsHlN-Ef"
      },
      "source": [
        "'''\n",
        "Info: Create randomly generated data (grayscale) and save it in folder /data (folder is created if not present)\n",
        "\n",
        "Input:\n",
        "  -nr_data (optional): number of randomly generated data\n",
        "\n",
        "Output: tuple(list[location], list[img_tensor])\n",
        "  -data_X: List of he x locations of the images\n",
        "  -data_I: Images as tensor\n",
        "\n",
        "'''\n",
        "\n",
        "def create_data_v2(nr_data = 100, noise = True):\n",
        "    plt.style.use('dark_background') #Change the style of ALL plots to black background [1]\n",
        "    data_X = []\n",
        "    data_I = []\n",
        "    \n",
        "    #Create random data\n",
        "    for i in range(0,nr_data,1):\n",
        "        I,X = create_tensor_v2(os.getcwd()+'/data/true_image'+str(i), noise = noise) #location is excluded to get random locations\n",
        "        data_X.append(torch.FloatTensor([X]))\n",
        "        data_I.append(I/255) # The I/255 is a conversion from RGB to grayscale \n",
        "        plt.close('all') #Close all plots and/or images (precaution for memory build up)\n",
        "    return (data_X, data_I)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e1zU11BQLOI"
      },
      "source": [
        "'''\n",
        "Info: Neural network using 4 transposed convolutional layers to generate an image from a single horizontal positional location\n",
        "\n",
        "Input: Single x locational variable\n",
        "Output: 40 x 40 image\n",
        "\n",
        "'''\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.epoch_losses = []\n",
        "        self.test_losses = []\n",
        "        self.saved_lr = []\n",
        "        self.saved_batch_size = []\n",
        "        \n",
        "        # The decoder uses 4 layeres, where 2 have rectangular kernels \n",
        "        super(Net,self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1 , 16, (1,10)),  #input (*batch_size*,1,1,1) output (*batch_size*,16,1,10)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16 , 8, (10,1)), #input (*batch_size*,16,1,10) output (*batch_size*,8,10,10)\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8 , 4 , 9,stride=2), #input (*batch_size*,8,10,10) output (*batch_size*,4,27,27)\n",
        "            nn.ReLU(True),\n",
        "            #nn.Dropout(p=0.2),\n",
        "            nn.ConvTranspose2d(4 , 1 , 14), #input (*batch_size*,4,27,27) output (*batch_size*,1,40,40)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    '''\n",
        "    Info: Evaluate the current model (uses entire dataset as batch)\n",
        "    Input:\n",
        "      -test_data (optional): tuple(list[x_location], list[image])\n",
        "    Output:\n",
        "      -test_loss: Mean squared error loss of the test_data\n",
        "    '''\n",
        "    def eval_model(self,test_data=None):\n",
        "        model.eval()#set model to evaluation mode\n",
        "        loss_func = nn.MSELoss()\n",
        "\n",
        "        if test_data==None:\n",
        "            X,I = create_data_v2(self.batch_size)\n",
        "            test_data = (X,I)   \n",
        "\n",
        "        with torch.no_grad():\n",
        "            test_input = Variable(torch.stack(test_data[0]).view(len(test_data[0]),1,1,1)) \n",
        "            \n",
        "            test_output = model.decoder(test_input)\n",
        "            test_original = torch.stack(test_data[1]).view(len(test_data[1]),1,40,40)\n",
        "                       \n",
        "            loss = loss_func(test_output, test_original.type(torch.FloatTensor))\n",
        "            test_loss = loss.item()\n",
        "            return test_loss\n",
        "    \n",
        "    # Main function, call to run the model, trains and tests the current model\n",
        "        # Input:\n",
        "        # - data (optional): Training data as tuple (tensorlist locations, tensorlist images)\n",
        "        # - epoch (optional): Number of cycles to run over the training (and test) data\n",
        "        # - plot (optional): True/False, if True will plot information and prrogress every 10 epochs\n",
        "        # - batch_size (optional): Set the batch size of the data --> if batch_size incompatible with data_size, data_size will reduce to fit in full batches\n",
        "        # Output:\n",
        "        # - epoch_losses: List of train losses from each epoch\n",
        "        # - test_losses: List of test losses from each epoch\n",
        "\n",
        "    '''\n",
        "    Info: Train and test the neural network (testing data is created on top of the training data, 30%)\n",
        "      -The neural network uses a schedular decreasing learning rate over time (based on ReduceLROnPlateau)\n",
        "    Input:\n",
        "      -data (optional): training data as tuple(list[x-location],list[image])\n",
        "      -epochs (optional): number of cycles to run over the data\n",
        "      -plot: True/False for plotting progress during runtime (increases runtime)\n",
        "      -batch_size (optional): number of data to run trough before updating internal parameters\n",
        "      -hybrid: True/False for increasing batch_size during runtime (increase every 10 epochs)\n",
        "    Output:\n",
        "      -epoch_losses: Mean squared error loss of each cycle (mean of MSE of the batches)\n",
        "      -test_losses: Mean squared error loss of each cycle running oveer the test data\n",
        "    '''\n",
        "    def train_model(self, data=None, epochs=20 , plot=True, batch_size = 16, hybrid = True):\n",
        "        # Initialization\n",
        "        self.batch_size = batch_size\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor = .8)\n",
        "        loss_func = nn.MSELoss()        \n",
        "\n",
        "        # Create random data if none is given\n",
        "        if data is None:\n",
        "            X,I = create_data_v2(500)\n",
        "            data = (X,I)\n",
        "\n",
        "        # Create random evaluation/test data (with a ratio of about 30% of the total data)\n",
        "        X_eval, I_eval = create_data_v2(math.floor((len(data)/0.7) * 0.3),noise = False)\n",
        "        data_eval = (X_eval, I_eval)\n",
        "\n",
        "        \n",
        "        #train and test model\n",
        "        for epoch in range(epochs):\n",
        "            for param_group in optimizer.param_groups:\n",
        "                self.saved_lr.append(param_group['lr'])\n",
        "\n",
        "            if epoch%10 == 0 and hybrid:\n",
        "                self.batch_size+=1\n",
        "            self.saved_batch_size.append(self.batch_size) #save batch size\n",
        "\n",
        "            batch_losses = []\n",
        "\n",
        "            #shuffle train and test data at start of each epoch\n",
        "            comb_data = list(zip(data[0], data[1])) # Combine X,I (keep the X with its attached I when shuffeling)\n",
        "            comb_data_eval = list(zip(data_eval[0], data_eval[1]))\n",
        "            random.shuffle(comb_data)\n",
        "            random.shuffle(comb_data_eval)\n",
        "            X, I = zip(*comb_data)\n",
        "            X_eval, I_eval = zip(*comb_data)\n",
        "\n",
        "            model.train() #Set model to train mode\n",
        "\n",
        "            \n",
        "\n",
        "            for i in (range(math.floor(len(X)/self.batch_size))): # turn the data in to batches of batch_size (rounded downwards)\n",
        "                input = Variable(torch.stack(X[i * self.batch_size:(i + 1) * self.batch_size]).view(self.batch_size,1,1,1), requires_grad=True)\n",
        "                original = torch.stack(I[i * self.batch_size:(i + 1) * self.batch_size]).view(self.batch_size,1,40,40)\n",
        "                output = model.forward(input)\n",
        "              \n",
        "                #Forward pass\n",
        "                optimizer.zero_grad() #zero the gradient buffers\n",
        "                loss = loss_func(output, original.type(torch.FloatTensor)) # Mean Squared Error (MSE) loss\n",
        "               \n",
        "                #Backward pass\n",
        "                loss.backward()\n",
        "                optimizer.step()  #update optimizer\n",
        "            \n",
        "                batch_losses.append(loss.item()) \n",
        "                output = output.detach() #This is done for plotting purposes\n",
        "            test_loss = self.eval_model((X_eval, I_eval)) #Set model to evaluation mode\n",
        "            \n",
        "            self.test_losses.append(test_loss)\n",
        "            epoch_loss = mean(batch_losses)\n",
        "            self.epoch_losses.append(epoch_loss)\n",
        "        \n",
        "            scheduler.step(epoch_loss) #update schedular\n",
        "        \n",
        "            # Plot information about the training and testing of the model during runtim if plot is true, otherwise a single plot at the end is shown\n",
        "            if (epoch % 10 == 0 and plot==True and epoch !=0) or epoch == epochs-1:\n",
        "                clear_output(wait=True) #Clear output field\n",
        "                \n",
        "                #Indicate losses of current cycle\n",
        "                print('epoch [{}/{}]\\nepoch loss: {}\\ntest loss: {}\\n'.format(epoch+1,epochs,epoch_loss,test_loss))        \n",
        "\n",
        "                #Show all losses\n",
        "                print('Loss plot (excluding first 5 epochs)')\n",
        "                x1 = np.linspace(5,len(self.epoch_losses),len(self.epoch_losses)-5,endpoint=True)\n",
        "                x2 = np.linspace(5,len(self.test_losses),len(self.test_losses)-5,endpoint=True)\n",
        "                plt.plot(x1,self.epoch_losses[5::],'w',label='train')\n",
        "                plt.plot(x2,self.test_losses[5::],'r--',label='test', alpha = 0.5)\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.xlabel('epoch')\n",
        "                plt.ylabel('MSE')\n",
        "                plt.xlim(xmin=5)\n",
        "                plt.show()\n",
        "                plt.clf()\n",
        "\n",
        "                #Show first/a random data generation from the neural network (shows progress)\n",
        "                print('\\nVisualization\\n epoch: {}\\n batch_size: {}'.format(epoch,self.batch_size))\n",
        "                fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(3, 5))\n",
        "                ax[0].imshow(output[0][0].view(40,40), origin='upper', cmap='gray')\n",
        "                ax[0].set_title('predicted')\n",
        "                ax[0].axis('off')\n",
        "                ax[1].imshow(original[0][0].view(40,40), origin='upper', cmap='gray')\n",
        "                ax[1].set_title('original')\n",
        "                ax[1].axis('off')\n",
        "                plt.show()\n",
        "                plt.clf()\n",
        "        return self.epoch_losses, self.test_losses\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDH3AVmRaeuD"
      },
      "source": [
        "'''\n",
        "Info: Plot information (losses, learning rate, batch size) about trained neural network\n",
        "'''\n",
        "\n",
        "'''\n",
        "Input:\n",
        "  -model: trained neural network\n",
        "  -reduce_start (optional): Set starting epoch for the plot\n",
        "  -reduce_end (optional): Remove N amount of epochs from the end\n",
        "Output: --Plot of train and test losses --\n",
        "'''\n",
        "def visualize_learning(model, reduce_start = 0, reduce_end = 0):\n",
        "  #calculate amount of epochs\n",
        "  len_data = np.linspace(reduce_start,len(model.epoch_losses)-reduce_end,len(model.epoch_losses)-reduce_start-reduce_end,endpoint = True)\n",
        "  #plot train losses\n",
        "  plt.plot(len_data,model.epoch_losses[reduce_start:len(model.epoch_losses)-reduce_end], 'b', label = 'train')\n",
        "  #plot test losses\n",
        "  plt.plot(len_data,model.test_losses[reduce_start:len(model.test_losses)-reduce_end], 'r', label = 'test')\n",
        "\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('MSE loss')\n",
        "  plt.xlim(xmin=reduce_start)\n",
        "  plt.title('epoch losses: '+str(reduce_start) + ' - '+str(len(model.epoch_losses) - reduce_end))\n",
        "  plt.show()\n",
        "\n",
        "'''\n",
        "Input:\n",
        "  -model: trained neural network\n",
        "Output: --Plot of learning rate and a plot of batch sizes--\n",
        "'''\n",
        "def visualize_params(model):\n",
        "    x = np.linspace(0,len(model.saved_lr),len(model.saved_lr),endpoint=True)\n",
        "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
        "    ax1.plot(x, model.saved_lr)\n",
        "    ax1.set_title('learning rate')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    ax1.set_ylabel('lr')\n",
        "    ax1.set_ylim([0,0.00012])\n",
        "    ax2.plot(x, model.saved_batch_size)\n",
        "    ax2.set_title('batch size')\n",
        "    ax2.set_xlabel('epoch')\n",
        "    ax2.set_ylabel('size')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtpCaFXge8i8"
      },
      "source": [
        "Only run the next 2/3 cells if you want to create your own neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr4X4DKMcRB0"
      },
      "source": [
        "'''\n",
        "Info: Create training data for the neural network (large dataset recommended)\n",
        "  -Only run once\n",
        "'''\n",
        "train_data_large = create_data_v2(1600, noise=True)\n",
        "#train_data_medium = create_data_v2(800, noise = True)\n",
        "#train_data_small = create_data_v2(200, noise = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "878kszUmcu8u"
      },
      "source": [
        "'''\n",
        "Info: Create, run and save neural network\n",
        "  -There is a small possibility the network gets stuck\n",
        "'''\n",
        "model = Net() #create network\n",
        "\n",
        "#In the case of hybrid it is advisary to start with a small batch size\n",
        "train_loss, test_loss = model.train_model(data=train_data_large, batch_size=1, epochs=800, hybrid = True) #run network\n",
        "\n",
        "#Save trained neural network,\n",
        "torch.save(model, os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuO8OoERdw3g"
      },
      "source": [
        "'''\n",
        "Info: Show additional/more specific results\n",
        "'''\n",
        "model = torch.load(os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth')  \n",
        "visualize_learning(model,50,0)\n",
        "visualize_params(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzU4sblWegN7"
      },
      "source": [
        "'''\n",
        "Info: Save the states created by armAI (active inference)\n",
        "\n",
        "Input:\n",
        "  -armAI: The armAI containing the values needed for plotting\n",
        "  -itt: The current itteration/state\n",
        "  -visual: True/False for plotting sensory visual state\n",
        "  -proprioception: True/False for plotting sensory proprioception state\n",
        "  -goal: True/False for plotting goal state\n",
        "Output: -- image of the armAI state --\n",
        "'''\n",
        "\n",
        "def save_state_v4(armAI, itt, visual, proprioception, goal):\n",
        "\n",
        "    legend = []\n",
        "    #get image to be plotted as indicators\n",
        "    image = mpimg.imread('arm.png')\n",
        "\n",
        "    #Create figure\n",
        "    fig = plt.figure(figsize=(2,1.5),frameon=False)\n",
        "\n",
        "    # Create environment\n",
        "    env = fig.add_axes([0, 0, 2.7, 1.5],alpha=0.5,facecolor='white') # The 2.7 is the -1,1 environment + 0.7 of arm width (that the arm at outer location -1 and 1 are still within the environment)\n",
        "    env.get_yaxis().set_visible(False)\n",
        "    env.get_xaxis().set_visible(False)\n",
        "\n",
        "    #Add belief/mental (blue)\n",
        "    mental_x_clipped = np.clip(armAI.mental_x,-1,1)\n",
        "    belief = fig.add_axes([(mental_x_clipped+1), 0, 0.7, 1],facecolor = 'b')\n",
        "    belief.patch.set_alpha(0.2)\n",
        "    belief.get_yaxis().set_visible(False)\n",
        "    belief.get_xaxis().set_visible(False)\n",
        "    belief.spines['bottom'].set_color('b')\n",
        "    belief.spines['top'].set_color('b') \n",
        "    belief.spines['right'].set_color('b')\n",
        "    belief.spines['left'].set_color('b')\n",
        "    belief.patch.set_alpha(0.5)\n",
        "    belief_leg = mpatches.Patch(color='b', label='Mental | ' + str(round(armAI.mental_x.item(), 3)))\n",
        "    legend.append(belief_leg)\n",
        "    \n",
        "    #Use the generated mental state as plot for belief\n",
        "    belief_img = armAI.network.decoder(Variable(armAI.mental_x.view(-1,1,1,1), requires_grad=False)).detach()\n",
        "    env.imshow(belief_img.view(40,40),cmap = 'gray',aspect='auto')\n",
        "\n",
        "    #Use the arm.png as plot for belief\n",
        "    #belief.imshow(image, aspect='auto', alpha=0.5)\n",
        "\n",
        "    # Add visual arm (red)\n",
        "    if visual:\n",
        "       visual_x_clipped = np.clip(armAI.visual_x,-1,1) #prevent plotting out of bounds\n",
        "       vis = fig.add_axes([(visual_x_clipped+1), 0, 0.7, 1],facecolor='r') # the +1 as the environment runs from 0 - 2\n",
        "       vis.patch.set_alpha(0.2) #give 'faint' background collor\n",
        "       vis.imshow(image, aspect='auto',alpha=0.8)\n",
        "       vis.get_yaxis().set_visible(False)\n",
        "       vis.get_xaxis().set_visible(False)\n",
        "       vis.spines['bottom'].set_color('red')\n",
        "       vis.spines['top'].set_color('red') \n",
        "       vis.spines['right'].set_color('red')\n",
        "       vis.spines['left'].set_color('red')\n",
        "       vis.patch.set_alpha(0.5)\n",
        "       vis_leg = mpatches.Patch(color='red', label='Visual | ' + str(round(armAI.visual_x.item(),3)))\n",
        "       legend.append(vis_leg)\n",
        "\n",
        "    #Add proprioception circle (yellow)\n",
        "    if proprioception: \n",
        "       prop_x_clipped = np.clip(armAI.proprioception_loc,-1,1) #prevent plotting out of bounds\n",
        "       prop = fig.add_axes([(prop_x_clipped+1), 0, 0.7, 1]) # the +1 as the environment runs from 0 - 2\n",
        "       circ = Circle((0.35,0.5),0.1, color = 'yellow')\n",
        "       prop.add_patch(circ)\n",
        "       prop.patch.set_alpha(0.5)\n",
        "       prop.axis('off')\n",
        "       prop_leg = mpatches.Patch(color='yellow', label='Prop | ' + str(round(armAI.proprioception_loc.item(),3)))\n",
        "       legend.append(prop_leg)\n",
        "    \n",
        "    #Add goal (green)\n",
        "    if goal: \n",
        "        goal_x_clipped = np.clip(armAI.goal_x,-1,1) #prevent plotting out of bounds\n",
        "        attr = fig.add_axes([(goal_x_clipped+1), 0, 0.7, 1],facecolor='g' )\n",
        "        attr.get_yaxis().set_visible(False)\n",
        "        attr.get_xaxis().set_visible(False)\n",
        "        attr.spines['bottom'].set_color('g')\n",
        "        attr.spines['top'].set_color('g') \n",
        "        attr.spines['right'].set_color('g')\n",
        "        attr.spines['left'].set_color('g')\n",
        "        attr.patch.set_alpha(0.5) \n",
        "        attr.imshow(image, aspect='auto', alpha=0.5)\n",
        "        attr.patch.set_alpha(0.2)\n",
        "        attr_leg = mpatches.Patch(color='g', label='Goal | ' + str(round(armAI.goal_x.item(),3)))\n",
        "        legend.append(attr_leg)\n",
        "\n",
        "    #Set title and legend\n",
        "    env.text(.5,.9, 'State: ' + str(itt) , horizontalalignment='center', transform=env.transAxes, color = 'white')\n",
        "    env.legend(handles=legend, prop={'size': 10})\n",
        "\n",
        "    plt.savefig(os.getcwd()+'/data/state: '+ str(itt) + '.png', bbox_inches='tight')\n",
        "    return fig\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG1Gl63gjfvt"
      },
      "source": [
        "'''\n",
        "Info: Helper functions for armAI (preventing excessive large cells)\n",
        "'''\n",
        "\n",
        "class AI_helper():\n",
        "    def __init__(self, armAI):\n",
        "      self.armAI = armAI\n",
        "\n",
        "    #Info: check for neural network, load 'trained_network_DataLarge_Hybrid' if none given\n",
        "    def check_network(self):\n",
        "      if self.armAI.network == None:\n",
        "          print('No neural network was given. Searching in folder networks for \\'trained_network_DataLarge_Hybrid.pth\\'')\n",
        "          if os.path.exists(os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth') == False:\n",
        "              print('No network found, please run the \\'Train, Test and Save model\\' section ')\n",
        "              sys.exit()\n",
        "          return torch.load(os.getcwd()+'/networks/trained_network_DataLarge_Hybrid.pth')    \n",
        "      else:\n",
        "          return self.armAI.network\n",
        "    \n",
        "    #Info: Check if the initialization is correct\n",
        "    def check_initialization(self):\n",
        "        if len(self.armAI.visual) == 0 or self.armAI.mental_x == None or (self.armAI.induce_movement and len(self.armAI.attractor_img) ==  0):\n",
        "            print('\\nINITIALIZATION INCOMPLETE: check set_mental(), set_visual() and/or set_attractor()')\n",
        "            print('vis: {}\\nmental: {}\\nattr: {}'.format(len(self.armAI.visual), self.armAI.mental_x , len(self.attractor_img)))\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    #Info: Create a new visual image based on new_location\n",
        "    def update_visual_state(self, new_location):\n",
        "        self.armAI.visual_prop = new_location\n",
        "        img,loc = create_tensor_v2('visual_state', new_location, noise = False)\n",
        "        self.armAI.set_visual(torch.FloatTensor([loc]),img/255)\n",
        "        plt.clf()\n",
        "\n",
        "    #Info: Add random normal Gaussian noise to value    \n",
        "    def add_noise(self, value, noise):\n",
        "        return value + np.random.normal(0, noise)\n",
        "\n",
        "    #Info: Reform the mu data and visual states as .gif\n",
        "    def save_gif(self, name, action):\n",
        "        img_list = []\n",
        "        for i in range(len(self.armAI.mu_dot_data)):\n",
        "            plt.style.use('classic')\n",
        "            fig, ax = plt.subplots(nrows=2, figsize = (10,9))\n",
        "            ax[1].plot(range(i), self.armAI.mu_dot_data[0:i], 'r', label='mu_dot | ' + str(round(self.armAI.mu_dot_data[i].item(),3)))\n",
        "            if action:\n",
        "                ax[1].plot(range(i), self.armAI.a_dot_data[0:i], 'g--', label = 'a_dot | ' + str(round(self.armAI.a_dot_data[i].item(),3)))\n",
        "            ax[1].set_xlabel('itteration')\n",
        "            ax[1].set_title('mu')\n",
        "            ax[1].legend(loc='lower center', fontsize='x-large')\n",
        "            ax[1].plot(range(i),np.zeros(i),'b--')\n",
        "\n",
        "            state = Image.open(os.getcwd()+'/data/state: '+ str(i) + '.png')\n",
        "            ax[0].imshow(state)\n",
        "            ax[0].set_xlabel('location')\n",
        "            ax[0].set_title('Environment')\n",
        "            ax[0].axis('off')\n",
        "\n",
        "            fig.canvas.draw()\n",
        "            image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
        "            image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            plt.close(fig)\n",
        "\n",
        "            img_list.append(image)\n",
        "\n",
        "\n",
        "        kwargs_write = {'fps':4.0, 'quantizer':'nq'}\n",
        "        imageio.mimsave(str(name) + '.gif', img_list, fps=4)\n",
        "        #img_list[0].save('temp.gif', save_all = True, optimize = False, duration = 100, loop = 0)\n",
        "        #img_list[0].save('States_all_v2.gif', save_all=True, append_images=img_list[1:], optimize=False, duration=2000, loop=0)\n",
        "\n",
        "          \n",
        "\n",
        "      \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUnOf-XKm0F0"
      },
      "source": [
        "'''\n",
        "Info: armAI, active inference | Using 2 sensory inputs, internal state (and goal state) to minimize prediction errors.\n",
        "*Note* somehow the vision is reversed, unable to find the cause..\n",
        "  -perceptual minimization\n",
        "  -action minimization\n",
        "  --goal minimization\n",
        "\n",
        "Input:\n",
        "  -Starting values\n",
        "  -limitations\n",
        "\n",
        "Output: -- .gif containing |all states| process of reaching equilibrium\n",
        "'''\n",
        "class ArmAI():\n",
        "    def __init__(self,max_itt=100, network = None, dt = 0.1):\n",
        "        \n",
        "      #INITIALIZE CLASS VARIABLES\n",
        "        self.dt = dt\n",
        "        self.max_itt = max_itt\n",
        "        self.network = network\n",
        "        self.AI_helper = AI_helper(self)\n",
        "        self.finished = False\n",
        "        \n",
        "        #Sigma's\n",
        "        self.sigma_vis= 0.1\n",
        "        self.sigma_prop = 0.95\n",
        "        self.sigma_dyn = 0.95\n",
        "        self.sigma_dyn_prop = 1\n",
        "        \n",
        "        #adap sigma\n",
        "        self.adapt_sigma_vis = True \n",
        "        self.sigma_v_gamma = 0.1\n",
        "\n",
        "        #Gain's\n",
        "        self.mu_gain = 0.1\n",
        "        self.a_gain = 0.2\n",
        "\n",
        "        self.action_gain = 0.1\n",
        "        self.perception_gain = 0.1\n",
        "        self.goal_gain = 0.05\n",
        "\n",
        "        self.visual_gain = 10\n",
        "        \n",
        "        #Error data (plotting purposes)\n",
        "        self.error_vis = [] \n",
        "        self.error_prop = []\n",
        "        self.error_mental = []\n",
        "        self.error_mental_prop = []\n",
        "        self.mu_dot_data = []\n",
        "        self.a_dot_data = []\n",
        "\n",
        "        #Visual (perception)\n",
        "        self.visual_img = torch.FloatTensor() #visual_state\n",
        "        self.visual_x = 0 #visual_location (plotting purposes)\n",
        "        self.pred_err_vis = torch.FloatTensor()\n",
        "        self.pred_err_vis_prior = torch.FloatTensor()\n",
        "\n",
        "        #Proprioception (perception)\n",
        "        self.proprioception_loc = 0\n",
        "\n",
        "        #Mental/belief\n",
        "        self.mental_img = torch.FloatTensor() #mental state\n",
        "        self.mental_x = 0 #mental location/proprioceptive location\n",
        "\n",
        "        #Attractor/goal\n",
        "        self.goal_img = torch.FloatTensor()\n",
        "        self.goal_x = 0 #attractor location\n",
        "      \n",
        "      #CHECK FOR NETWORK\n",
        "        #initialize network\n",
        "        self.network = self.AI_helper.check_network() \n",
        "        #set network to evaluation mode\n",
        "        self.network.eval()\n",
        "        \n",
        "    #SET CLASS VARIABLES\n",
        "    def set_mental(self,belief_loc): #Mental/Belief position\n",
        "        self.mental_x = belief_loc\n",
        "        \n",
        "    def set_goal(self, goal_loc, goal_vis):#Goal position\n",
        "        self.goal_vis = goal_vis\n",
        "        self.goal_x = goal_loc \n",
        "\n",
        "    def set_visual(self, visual_x, visual_img):#Sensory visual\n",
        "        self.visual_img = visual_img\n",
        "\n",
        "        self.visual_x = visual_x #(plotting/visualizing purposes)\n",
        "\n",
        "    def set_proprioception(self, prop_loc): #Sensory proprioception\n",
        "        self.proprioception_loc = prop_loc\n",
        "    \n",
        "    #Visual error (error based on vision)\n",
        "    def visual_error(self, input_image):\n",
        "        #Generate mental/belief image\n",
        "        input = Variable(self.mental_x.view(-1,1,1,1), requires_grad=True)           \n",
        "        self.mental_img = self.network.decoder(input)\n",
        "    \n",
        "        err_vis = (input_image - self.mental_img.detach()) #prediction error\n",
        "        err_vis_var = torch.var(err_vis) #Variance, Note that precision is the inversed variance\n",
        "        #Error visual\n",
        "        #err_vis_var = np.clip(err_vis_var,0.0014,1.5)\n",
        "        error_vis = 1/err_vis_var * (input_image - self.mental_img) \n",
        " \n",
        "        #Backward pass\n",
        "        input.grad = torch.zeros(input.size())\n",
        "        self.mental_img.backward(0.1 * error_vis, retain_graph=True)\n",
        "\n",
        "        return input.grad\n",
        "\n",
        "    #Locational/proprioception error (error based on horizontal locations)\n",
        "    def prop_error(self,input_prop):\n",
        "        error_prop = (1/0.05) * (input_prop - self.mental_x) #Precision * prediciton error\n",
        "        return error_prop   \n",
        "\n",
        "    '''\n",
        "    Info: Active inference with the capability of moving towards a goal\n",
        "    Input: based on initialization\n",
        "      -perception (optional):True/False for using perception to minimize variational free energy\n",
        "      -action (optional): True/False for using action to minimize variational free energy\n",
        "      -sense_vis (optional): True/False for using sensory vision\n",
        "      -sense_prop (optional): True/False for using sensory proprioception \n",
        "      -goal (optional): True/False for moving towards a goal state\n",
        "      -name (optional): Name of the resulting .gif\n",
        "    Output: -- a name.gif containing the progress --\n",
        "    '''\n",
        "    def active_inference(self, perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'armAI'):\n",
        "      \n",
        "      for i in range(self.max_itt):\n",
        "          #Reset variables and save current state\n",
        "          save_state_v4(self, i, sense_vis, sense_prop, goal)\n",
        "          mu_vis = 0\n",
        "          mu_prop = 0\n",
        "          mu_action_vis = 0\n",
        "          mu_action_prop = 0\n",
        "          mu_goal_vis = 0 #preferred state\n",
        "          mu_goal_prop = 0 #preferred state\n",
        "\n",
        "          #Perception part of minimizing surprise\n",
        "          if perception:\n",
        "            if sense_vis:\n",
        "              mu_vis =  -1 * self.visual_error(self.visual_img)\n",
        "            if sense_prop:\n",
        "              self.proprioception_loc_noise = self.AI_helper.add_noise(self.proprioception_loc, 0)\n",
        "              mu_prop = self.prop_error(self.proprioception_loc_noise)\n",
        "          #Action part of minimizing surprise\n",
        "          if action:\n",
        "            if goal: #This would be an inferred action that would reduce future free energy (expected free energy [https://arxiv.org/pdf/2004.08128.pdf])\n",
        "              #goal is not a sensory state, thus needs to be generated trough the neural network to compare with mental state\n",
        "              mu_goal_vis = -1 * self.visual_error(self.goal_vis)\n",
        "              mu_goal_prop =  self.prop_error(self.goal_x)\n",
        "\n",
        "            #Variational free energy\n",
        "            if sense_vis:\n",
        "                #error sensory to mental (-1 * error mental to sensory)\n",
        "                mu_action_vis =  self.visual_error(self.visual_img)\n",
        "                 \n",
        "            if sense_prop:\n",
        "                self.proprioception_loc_noise = self.AI_helper.add_noise(self.proprioception_loc, 0)\n",
        "                #error proprioception to mental (-1 * error mental to sensory)\n",
        "                mu_action_prop = (-1) *self.prop_error(self.proprioception_loc_noise) \n",
        "\n",
        "          #Sum the prediction errors\n",
        "          a_dot =    mu_action_vis +  mu_action_prop #action 'velocity'\n",
        "          if goal:\n",
        "            mu_dot =    mu_goal_vis + mu_goal_prop #mental 'velocity' (mu_goal_vis/prop = 0 if goal=False)\n",
        "          else:\n",
        "            mu_dot =    mu_vis+  mu_prop \n",
        "\n",
        "          #print('mu_vis: {}\\n mu_prop: {}\\n mu_action_vis: {}\\n mu_action_prop: {}\\n mu_goal_vis: {}\\nmu_goal_prop: {}\\n\\n mu_dot: {}\\na_dot: {}\\n\\n'.format(mu_vis,mu_prop,mu_action_vis,mu_action_prop,mu_goal_vis,mu_goal_prop,mu_dot,a_dot))\n",
        "\n",
        "          #save data (plotting purposes)\n",
        "          a_dot = a_dot * self.a_gain\n",
        "          mu_dot = mu_dot * self.mu_gain\n",
        "          if action:\n",
        "            self.a_dot_data.append(a_dot)\n",
        "          self.mu_dot_data.append(torch.FloatTensor([mu_dot]))\n",
        "\n",
        "          #Update states\n",
        "          \n",
        "          new_vis_loc = torch.add(self.visual_x, a_dot, alpha=self.dt)\n",
        "          self.AI_helper.update_visual_state(new_vis_loc) #Update visual arm\n",
        "          self.proprioception_loc = torch.add(self.proprioception_loc,  a_dot, alpha=self.dt) #Update proprioceptive location\n",
        "          self.mental_x = torch.add(self.mental_x,  mu_dot, alpha=self.dt) #Update mental location\n",
        "          plt.close('all')\n",
        "\n",
        "\n",
        "          \n",
        "      #save all states to a gif\n",
        "      self.AI_helper.save_gif(name, action)\n",
        "    \n",
        "              \n",
        "            \n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcHkHmzPz5dy"
      },
      "source": [
        "The cell below contains test setups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0O1w8rpPg_M"
      },
      "source": [
        "'''\n",
        "## Base Case\n",
        "# -All variables are set to 0. Too much movement indicating variable adjustments need to be made\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0, noise = False)  #Create visual state\n",
        "mental_loc = 0 # Does not require an image\n",
        "proprioception_loc = 0\n",
        "goal_img,goal_loc = create_tensor_v2('init goal', location = 0, noise = False) \n",
        "\n",
        "armAI = ArmAI(max_itt = 50,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = True, name = 'Base_case')\n",
        "\n",
        "\n",
        "## Perception test\n",
        "#   -Check the model while only based on perception can perform as expected\n",
        "#     -Both visual and proprioception states are initialized at the same location. Mental state at a random location with the expectation to correct itself\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.7, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "goal_img,goal_loc = create_tensor_v2('init goal', location = 0, noise = False) \n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = False, sense_vis = True, sense_prop = True, goal = False, name = 'Perception_test')\n",
        "\n",
        "\n",
        "## Action test\n",
        "#   -Check the model while only based on action can perform as expected\n",
        "#     -Both visual and proprioception states are initilized at the same location. Mental state at a random location with the expectation of the sensory states to correct themselved\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.7, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "goal_img,goal_loc = create_tensor_v2('init goal', location = 0, noise = False) \n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = False, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'Action_test')\n",
        "\n",
        "\n",
        "## Goal test\n",
        "#   -Check if the model is capable of moving towards a goal state\n",
        "#     -Visual, proprioception and mental states are initialized at the same location. A goal state is set with the expectation that all finish at the goal state, with the mental state leading the way.\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.7, noise = False)  #Create visual state\n",
        "mental_loc = 0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "goal_img,goal_loc = create_tensor_v2('init goal', location = -0.4, noise = False) \n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = True, name = 'Goal_test')\n",
        "\n",
        "\n",
        "\n",
        "#--- Special cases --\n",
        "\n",
        "## Rubbed hand illusion\n",
        "#   -The proprioception and mental state are initialized at the same location. The visual state is the rubber hand used in the RHI expriments, expecting that some mental discplacement will come fort\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.1, noise = False)  #Create visual state\n",
        "mental_loc = 0.5 # Does not require an image\n",
        "proprioception_loc = 0.5\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = False, sense_vis = True, sense_prop = True, goal = False, name = 'Rubber_hand_illusion')\n",
        "\n",
        "## Full random placement\n",
        "#   -The mental state and the proprioception state are expected to move towards eachother. \n",
        "#     However, in the case that the mental state and visual state find overlapment a visual error can be produced and taken into account.\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0, noise = False)  #Create visual state\n",
        "mental_loc = -0.8 # Does not require an image\n",
        "proprioception_loc = 0.8\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'Full_random_visMid')\n",
        "\n",
        "#----\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.5, noise = False)  #Create visual state\n",
        "mental_loc = 0.3 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = True, goal = False, name = 'Full_random_mentalMid')\n",
        "\n",
        "\n",
        "## Proprioception only\n",
        "#   - Active inference performed without Vision\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = -0.7, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "#goal_img,goal_loc = create_tensor_v2('init goal', location = 0.4, noise = False) \n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "armAI.set_goal(torch.FloatTensor([goal_loc]),goal_img/255)\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = False, sense_prop = True, goal = False, name = 'Prop_only')\n",
        "\n",
        "\n",
        "## Vision only\n",
        "#   - Active inference performed without proprioception (Losing Touch: A man without his body)\n",
        "#       -No overlap (with and wihout goal)\n",
        "#       -Overlap (with and without goal)\n",
        "\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.2, noise = False)  #Create visual state\n",
        "mental_loc = -0.7 # Does not require an image\n",
        "proprioception_loc = 0.7\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 80,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = False, goal = False, name = 'VisOnly_NoOverlap')\n",
        "\n",
        "#-----\n",
        "'''\n",
        "visual_img, visual_loc = create_tensor_v2('init vision', location = 0.6, noise = False)  #Create visual state\n",
        "mental_loc = 0.8 # Does not require an image\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "\n",
        "armAI = ArmAI(max_itt = 100,dt = 0.05)\n",
        "\n",
        "armAI.set_visual(torch.FloatTensor([visual_loc]),visual_img/255)\n",
        "armAI.set_mental(torch.FloatTensor([mental_loc]))\n",
        "armAI.set_proprioception(torch.FloatTensor([proprioception_loc]))\n",
        "\n",
        "\n",
        "armAI.active_inference(perception = True, action = True, sense_vis = True, sense_prop = False, goal = False, name = 'VisOnly_Overlap')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}